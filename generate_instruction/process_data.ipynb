{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "import sys\n",
    "from vllm import LLM\n",
    "\n",
    "sys.path.append(\"/mnt/bn/data-tns-live-llm/leon/Cherry_LLM\")\n",
    "from niid_data.utils import process_sft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset sahil2801/CodeAlpaca-20k has 20022 examples. =====\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "client_data = process_sft_dataset(\"sahil2801/CodeAlpaca-20k\",code_data)\n",
    "iid_idxs = random.sample(range(len(client_data)), 1000)\n",
    "base_data = client_data.select(iid_idxs)\n",
    "clients_data = []\n",
    "for i in range(10):\n",
    "    clients_data.append(base_data.shard(10,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_gpt(prompt, format):\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\":  [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant designed to output JSON. The format is: {format}\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"stop\": None,\n",
    "        \"max_tokens\": 512,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"user\": None,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "    data = {k: v for k, v in data.items() if v is not None}\n",
    "    data = json.dumps(data)\n",
    "    url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unsloth 本地运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-26 17:45:56,359] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "model: PeftModelForCausalLM\n",
    "tokenizer: LlamaTokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-Instruct-bnb-4bit\",dtype = torch.bfloat16,\n",
    "    load_in_4bit = True)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_completion(instance, prefix_function):\n",
    "    inputs = tokenizer(instance, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    generate_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024, repetition_penalty=1.1, do_sample=True, prefix_allowed_tokens_fn=prefix_function, min_new_tokens=50)\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "from transformers import pipeline\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    Instruction: str\n",
    "\n",
    "# Create a transformers pipeline\n",
    "# hf_pipeline = pipeline('text-generation', model='TheBloke/Llama-2-7b-Chat-GPTQ', device_map='auto')\n",
    "prompt = f\"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: \n",
    "Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. \n",
    "Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7 object2 = 8 \n",
    "Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" \n",
    "Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] \n",
    "Provide a new instruction in the following json schema: {AnswerFormat.schema_json()} :\\n\"\"\"\n",
    "\n",
    "# Create a character level parser and build a transformers prefix function from it\n",
    "parser = JsonSchemaParser(AnswerFormat.schema())\n",
    "prefix_function = build_transformers_prefix_allowed_tokens_fn(tokenizer, parser)\n",
    "\n",
    "# Call the pipeline with the prefix function\n",
    "output_dict = generate_one_completion(prompt, prefix_function)\n",
    "# Extract the results\n",
    "result = output_dict[len(prompt):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a transformers pipeline\n",
    "hf_pipeline = pipeline('text-generation', model='TheBloke/Llama-2-7b-Chat-GPTQ', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerFormat(BaseModel):\n",
    "    Instruction: str\n",
    "\n",
    "prompt = f\"\"\"You are asked to come up with instructions. Don't repeat instructions in examples. Here are some examples: \n",
    "Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. \n",
    "Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7 object2 = 8 \n",
    "Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" \n",
    "Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] \n",
    "Provide a new instruction in the following json schema: {AnswerFormat.schema_json()} :\\n\"\"\"\n",
    "\n",
    "parser = JsonSchemaParser(AnswerFormat.schema())\n",
    "prefix_function = build_transformers_prefix_allowed_tokens_fn(hf_pipeline.tokenizer, parser)\n",
    "output_dict = hf_pipeline(prompt, prefix_allowed_tokens_fn=prefix_function)\n",
    "\n",
    "# Extract the results\n",
    "result = output_dict[0]['generated_text'][len(prompt):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm 本地运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/mnt/bn/data-tns-live-llm/leon/datasets/Meta-Llama-3-8B-Instruct/\", tensor_parallel_size=1, \n",
    "    dtype=torch.bfloat16, trust_remote_code=True, enable_lora=False, max_model_len=512, gpu_memory_utilization=0.8)\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [\"USER:\", \"ASSISTANT:\", \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\", \"<|eot_id|>\", \"####\",\"}\"]\n",
    "sampling_params = SamplingParams(top_p=1, max_tokens=512, repetition_penalty=1.1, stop=stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s, est. speed input: 537.44 toks/s, output: 78.96 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "{\"Instruction\": \"Write a code snippet that uses the Euclidean algorithm to find the greatest common divisor (GCD) of two input integers.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: \n",
    "Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. \n",
    "Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7 object2 = 8 \n",
    "Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" \n",
    "Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] \n",
    "Provide a new instruction below. Output should in json format like this {\"Instruction\": instruction}, don't output any other information.\"\"\", sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_stop_tokens = [\"USER:\", \"ASSISTANT:\", \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\",\"<|eot_id|>\"]\n",
    "res_sampling_params = SamplingParams(top_p=1, max_tokens=512, repetition_penalty=1.1)\n",
    "outputs = llm.generate(\"\"\"Example 1: \\n    Instruction: Write an algorithm that prints the first 10 Fibonacci numbers in descending order.\\n    Response: # Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)\\nExample 2: \\n    Instruction: Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30\\n    Response: UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;\\n\n",
    "Generate the response of this instruction, Instruction: \"Write a code snippet that uses the Euclidean algorithm to find the greatest common divisor (GCD) of two input integers.\"\n",
    "Output should in json format like this {\"Response\": \\\"\\\"\\\"response\\\"\\\"\\\"}<\\END>, don't output any other information.\"\"\", res_sampling_params)\n",
    "print(outputs[0].outputs[0].text.split(\"<\\END>\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试 vllm json mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_schema = \"\"\"{\n",
    "    \"title\": \"Instruction Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Instruction\": {\n",
    "            \"title\": \"instruction\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Instruction\"]\n",
    "}\"\"\"\n",
    "\n",
    "response_schema = \"\"\"{\n",
    "    \"title\": \"Response Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Response\": {\n",
    "            \"title\": \"response\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Response\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"damn\"\n",
    ")\n",
    "\n",
    "def get_vllm(prompt, schema):\n",
    "    num_tokens=0\n",
    "    if \"Response\" in schema: \n",
    "        num_tokens=50\n",
    "\n",
    "    cnt=0\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        output = None\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"Meta-Llama-3-8B-Instruct/\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a helpful assistant designed to output JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"repetition_penalty\": 1.1,\n",
    "                \"min_tokens\": num_tokens,\n",
    "                \"guided_json\": schema\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            output = json.loads(completion.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"gpt error {e}\")\n",
    "            print(f\"gpt output {completion}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlines generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from outlines import models, generate\n",
    "\n",
    "class InstructionSchema(BaseModel):\n",
    "    Instruction: str\n",
    "\n",
    "model = models.transformers(\"/mnt/bn/data-tns-live-llm/leon/datasets/Meta-Llama-3-8B-Instruct/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 28/28 [00:01<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction='Insert a value x at the end of a list. givenList: [1, 2, 3, 4, 5], x: 6'\n"
     ]
    }
   ],
   "source": [
    "class InstructionSchema(BaseModel):\n",
    "    Instruction: str\n",
    "    \n",
    "generator = generate.json(model, InstructionSchema)\n",
    "result = generator(\n",
    "    \"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7\n",
    "object2 = 8 Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] Provide a new instruction below:\"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction='Write a function in JavaScript that returns the longest common prefix among all the elements of an array of strings.'\n"
     ]
    }
   ],
   "source": [
    "result = generator(\n",
    "    \"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7\n",
    "object2 = 8 Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] Provide a new instruction below:\"\"\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "response_examples = []\n",
    "for i in range(100):\n",
    "    instructions.append(random.sample(clients_data[0][\"instruction\"],4))\n",
    "    response_examples.append(clients_data[0].select(random.sample(range(len(clients_data[0])),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "def compute_sim(instruction):\n",
    "    return rouge.compute(predictions=[instruction], references=[clients_data[0][\"instruction\"]],rouge_types=['rougeL'])[\"rougeL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: Instruction 1: {} Instruction 2: {} Instruction 3: {} Instruction 4: {} Provide a new instruction below:\"\"\"\n",
    "\n",
    "instruction_format = \"{Instruction: instruction}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction(prompt, instruction_format=instruction_format):\n",
    "    cnt=0\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        instruction = get_vllm(prompt, instruction_schema)\n",
    "        # try:\n",
    "        #     # instruction = json.loads(get_gpt(prompt,instruction_format))[\"Instruction\"]\n",
    "        #     instruction = get_vllm(prompt, instruction_schema)[\"Instruction\"]\n",
    "        # except:\n",
    "        #     print(\"generate instruction error\")\n",
    "        #     continue\n",
    "        # if compute_sim(instruction)>0.7: \n",
    "        #     print(instruction)\n",
    "        #     continue\n",
    "        break\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format=\"{Response: response}\"\n",
    "\n",
    "def generate_response(prompt, response_format=response_format):\n",
    "    cnt=0\n",
    "    output = None\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        # output = get_gpt(prompt,response_format)\n",
    "        output = get_vllm(prompt,response_schema)\n",
    "        break\n",
    "        # try:\n",
    "        #     # output = json.loads(output)[\"Response\"]\n",
    "        #     output = output[\"Response\"]\n",
    "        #     break\n",
    "        # except:\n",
    "        #     print(output)\n",
    "        #     # time.sleep(1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7\n",
      "object2 = 8 Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] Provide a new instruction below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Write a Python function that takes a string as input and returns the string with all vowels removed.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = instruction_prompt.format(*instructions[0])\n",
    "generate_instruction(tmp,instruction_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = \"\"\"\n",
    "Example 1: \n",
    "    Instruction: {}\n",
    "    Response: {}\n",
    "Example 2: \n",
    "    Instruction: {}\n",
    "    Response: {}\n",
    "\n",
    "Generate the response of this instruction, Instruction: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response_prompt(idx, instruction):\n",
    "    response_seqs = []\n",
    "    for data in response_examples[idx]:\n",
    "        response_seqs.extend([data[\"instruction\"],data[\"response\"]])\n",
    "    response_seqs.append(instruction)\n",
    "    prompt = response_format.format(*response_seqs)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write an algorithm that prints the first 10 Fibonacci numbers in descending order.', '# Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)', 'Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30', 'UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;']\n"
     ]
    }
   ],
   "source": [
    "response_seqs = []\n",
    "for data in response_examples[0]:\n",
    "    response_seqs.extend([data[\"instruction\"],data[\"response\"]])\n",
    "print(response_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample 1: \\n    Instruction: Write an algorithm that prints the first 10 Fibonacci numbers in descending order.\\n    Response: # Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)\\nExample 2: \\n    Instruction: Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30\\n    Response: UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;\\n\\nGenerate the response of this instruction, Instruction: Write a Python function that takes a string as input and returns the string with all vowels removed.\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a Python function that takes a string as input and returns the string with all vowels removed.\"\n",
    "response_seqs.append(instruction)\n",
    "tmp_prompt = response_format.format(*response_seqs)\n",
    "tmp_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def remove_vowels(input_string):\\\\\\\\n    vowels = 'aeiouAEIOU'\\\\\\\\n    output_string = ''\\\\\\\\n    for char in input_string:\\\\\\\\n        if char not in vowels:\\\\\\\\n            output_string += char\\\\n    return output_string\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(tmp_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:01<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "gen_instructions = []\n",
    "gen_responses = []\n",
    "for i in tqdm(range(50)):\n",
    "    tmp = instruction_prompt.format(*instructions[i])\n",
    "    instruction = generate_instruction(tmp,instruction_format)\n",
    "    prompt = format_response_prompt(i,instruction)\n",
    "    response = generate_response(prompt)\n",
    "    gen_instructions.append(instruction)\n",
    "    gen_responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_instruction_prompt = [instruction_prompt.format(*instruction) for instruction in instructions]\n",
    "formatted_instruction_prompt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_instructions = generate_instruction(formatted_instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"instruction\": gen_instructions,\n",
    "    \"response\": gen_responses,\n",
    "})\n",
    "df.to_csv(f\"./code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 月之暗面！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    api_key=\"sk-Nl2yxo0Ic9ZJ0zFf3qWdypuoQ6Nd2xnsCp7wfojlW9OyOYmq\", # 在这里将 MOONSHOT_API_KEY 替换为你从 Kimi 开放平台申请的 API Key\n",
    "    base_url=\"https://api.moonshot.cn/v1\",\n",
    ")\n",
    "\n",
    "# {\n",
    "#     \"response\": \"text\",\n",
    "# }\n",
    "system_prompt = \"\"\"\n",
    "You are the intelligent customer service of Dark Side of the Moon (Kimi), and you are responsible for answering various questions raised by users. Please refer to the documentation to reply to the user's question. Your answer can be text, pictures, and links, and you can include text, pictures, and links in one reply.\n",
    "Please use the following JSON format to output your response:\n",
    " \n",
    "{\n",
    "    \"Task 1\": \"instruction\",\n",
    "    \"Task 2\": \"instruction\",\n",
    "    \"Task 3\": \"instruction\",\n",
    "    \"Task 4\": \"instruction\",\n",
    "    \"Task 5\": \"instruction\",\n",
    "}\n",
    "\"\"\"\n",
    " \n",
    "completion = client.chat.completions.create(\n",
    "    model=\"moonshot-v1-8k\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are Kimi, an artificial intelligence assistant provided by Moonshot AI. You are better at conversation in Chinese and English. You will provide users with safe, helpful and accurate answers. At the same time, you will reject all answers involving terrorism, racism, pornographic violence, etc. Moonshot AI is a proper noun and cannot be translated into other languages.\"},\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, # <-- 将附带输出格式的 system prompt 提交给 Kimi\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    response_format={\"type\": \"json_object\"}, # <-- 使用 response_format 参数指定输出格式为 json_object\n",
    ")\n",
    " \n",
    "content = json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Task 1': \"The sentiment of the news 'The goal is to secure the company's profitability and competitiveness' can be considered as positive, as it implies a forward-looking and optimistic approach to business growth.\", 'Task 2': \"The sentiment of the news about Apple's first foray into live sports broadcasting can be seen as moderately positive, as it indicates a new venture and potential for growth in a new market segment.\", 'Task 3': 'The sentiment of the news about semiconductor stocks falling and nearing bear-market territory is moderately negative, as it suggests a downturn in the market and potential financial concerns for investors.', 'Task 4': \"The sentiment of the tweet about Iberchem's record year and growth in revenue is positive, as it highlights successful business performance and expansion.\", 'Task 5': 'The sentiment of the news about Costco being watched by Zacks.com users can be interpreted as mildly positive, as it suggests interest and potential positive attention towards the stock.', 'Task 6': \"The sentiment of the news about an electric vehicle specialist outpacing the growth of the world's largest company is strong positive, as it indicates significant success and rapid expansion in the industry.\", 'Task 7': 'The sentiment of the news about Neste Oil working with research communities to develop new raw materials is positive, as it shows collaboration and innovation efforts.', 'Task 8': 'The sentiment of the tweet about Xylem reporting earnings is neutral, as it is a factual statement about a routine business activity without any positive or negative connotation.', 'Task 9': 'There is no provided content for Task 9, so I cannot determine the sentiment of the news or tweet.'}\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75772\n"
     ]
    }
   ],
   "source": [
    "data_concated: Dataset = processed_data[0]\n",
    "random.seed(42)\n",
    "iid_idxs = random.sample(range(len(data_concated)), 1000)\n",
    "base_data = data_concated.select(iid_idxs)\n",
    "clients_data = []\n",
    "for i in range(10):\n",
    "    clients_data.append(base_data.shard(10,i))\n",
    "\n",
    "data_concated = data_concated.select(list(set(range(len(data_concated)))-set(iid_idxs)))\n",
    "print(len(data_concated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'response', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(clients_data[0].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，文件已保存为 output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "domain = \"financial\"\n",
    "# 定义转换函数，将数据集中的一行转换为所需的JSONL格式\n",
    "def convert_to_jsonl(i,row):\n",
    "    return {\n",
    "        \"id\": f\"seed_task_{i}\",\n",
    "        \"name\": row['name'] if 'name' in row else domain,  # 假设每行都有'name'列，如果没有，则使用默认值\n",
    "        \"instruction\": row['instruction'],\n",
    "        \"instances\": [{\"input\": \"\", \"output\": row['response']}],\n",
    "        \"is_classification\": False\n",
    "    }\n",
    "\n",
    "# 打开文件用于写入\n",
    "with open('output.jsonl', 'w') as f:\n",
    "    # 遍历数据集中的所有行\n",
    "    for i,item in enumerate(clients_data[0]):  # 假设我们处理的是训练集\n",
    "        # 转换为JSONL格式\n",
    "        jsonl_line = convert_to_jsonl(i,item)\n",
    "        # 将JSON对象转换为JSON字符串，并写入文件\n",
    "        f.write(json.dumps(jsonl_line) + '\\n')\n",
    "\n",
    "print(\"转换完成，文件已保存为 output.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
