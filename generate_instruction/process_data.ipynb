{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-28 13:44:45 cuda.py:22] You are using a deprecated `pynvml` package. Please install `nvidia-ml-py` instead. See https://pypi.org/project/pynvml for more information.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "import sys\n",
    "from vllm import LLM\n",
    "\n",
    "sys.path.append(\"/mnt/bn/data-tns-live-llm/leon/Cherry_LLM\")\n",
    "from niid_data.utils import process_sft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1971471f248545c795d2779db8ef7a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c488a3b4b9be4963b23f91885f26fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb461be32d234da39061db922e4b09b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaee1bc2f4c4663af98a5d6468067a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6845a3aac3c406e9bfd141c2134c917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ea6e10f98d43a5be793e7136950081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/262039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset sahil2801/CodeAlpaca-20k has 20022 examples. =====\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "client_data = process_sft_dataset(\"sahil2801/CodeAlpaca-20k\",code_data)\n",
    "iid_idxs = random.sample(range(len(client_data)), 1000)\n",
    "base_data = client_data.select(iid_idxs)\n",
    "clients_data = []\n",
    "for i in range(10):\n",
    "    clients_data.append(base_data.shard(10,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_gpt(prompt, format):\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"messages\":  [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant designed to output JSON. The format is: {format}\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"stop\": None,\n",
    "        \"max_tokens\": 512,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"user\": None,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "    data = {k: v for k, v in data.items() if v is not None}\n",
    "    data = json.dumps(data)\n",
    "    url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unsloth æœ¬åœ°è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-26 17:45:56,359] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "model: PeftModelForCausalLM\n",
    "tokenizer: LlamaTokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-Instruct-bnb-4bit\",dtype = torch.bfloat16,\n",
    "    load_in_4bit = True)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_completion(instance, prefix_function):\n",
    "    inputs = tokenizer(instance, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "    generate_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1024, repetition_penalty=1.1, do_sample=True, prefix_allowed_tokens_fn=prefix_function, min_new_tokens=50)\n",
    "    outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "from transformers import pipeline\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    Instruction: str\n",
    "\n",
    "# Create a transformers pipeline\n",
    "# hf_pipeline = pipeline('text-generation', model='TheBloke/Llama-2-7b-Chat-GPTQ', device_map='auto')\n",
    "prompt = f\"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: \n",
    "Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. \n",
    "Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7 object2 = 8 \n",
    "Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" \n",
    "Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] \n",
    "Provide a new instruction in the following json schema: {AnswerFormat.schema_json()} :\\n\"\"\"\n",
    "\n",
    "# Create a character level parser and build a transformers prefix function from it\n",
    "parser = JsonSchemaParser(AnswerFormat.schema())\n",
    "prefix_function = build_transformers_prefix_allowed_tokens_fn(tokenizer, parser)\n",
    "\n",
    "# Call the pipeline with the prefix function\n",
    "output_dict = generate_one_completion(prompt, prefix_function)\n",
    "# Extract the results\n",
    "result = output_dict[len(prompt):]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å°è¯• vllm json mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_schema = \"\"\"{\n",
    "    \"title\": \"Instruction Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Instruction\": {\n",
    "            \"title\": \"instruction\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Instruction\"]\n",
    "}\"\"\"\n",
    "\n",
    "response_schema = \"\"\"{\n",
    "    \"title\": \"Response Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Response\": {\n",
    "            \"title\": \"response\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Response\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"damn\"\n",
    ")\n",
    "\n",
    "def get_vllm(prompt, schema):\n",
    "    num_tokens=0\n",
    "    if \"Response\" in schema: \n",
    "        num_tokens=50\n",
    "\n",
    "    cnt=0\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        output = None\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"Meta-Llama-3-8B-Instruct/\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"You are a helpful assistant designed to output JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            extra_body={\n",
    "                \"repetition_penalty\": 1.1,\n",
    "                \"min_tokens\": num_tokens,\n",
    "                \"guided_json\": schema\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            output = json.loads(completion.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"gpt error {e}\")\n",
    "            print(f\"gpt output {completion}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç”Ÿæˆresponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "response_examples = []\n",
    "for i in range(100):\n",
    "    instructions.append(random.sample(clients_data[0][\"instruction\"],4))\n",
    "    response_examples.append(clients_data[0].select(random.sample(range(len(clients_data[0])),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)',\n",
       " 'UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_examples[0][\"instruction\"]\n",
    "response_examples[0][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Write an algorithm that prints the first 10 Fibonacci numbers in descending order.', 'response': '# Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)'}\n",
      "{'instruction': 'Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30', 'response': 'UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;'}\n"
     ]
    }
   ],
   "source": [
    "for data in response_examples[0]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "def compute_sim(instruction):\n",
    "    return rouge.compute(predictions=[instruction], references=[clients_data[0][\"instruction\"]],rouge_types=['rougeL'])[\"rougeL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: Instruction 1: {} Instruction 2: {} Instruction 3: {} Instruction 4: {} Provide a new instruction below:\"\"\"\n",
    "\n",
    "instruction_format = \"{Instruction: instruction}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction(prompt, instruction_format=instruction_format):\n",
    "    cnt=0\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        instruction = get_vllm(prompt, instruction_schema)\n",
    "        # try:\n",
    "        #     # instruction = json.loads(get_gpt(prompt,instruction_format))[\"Instruction\"]\n",
    "        #     instruction = get_vllm(prompt, instruction_schema)[\"Instruction\"]\n",
    "        # except:\n",
    "        #     print(\"generate instruction error\")\n",
    "        #     continue\n",
    "        # if compute_sim(instruction)>0.7: \n",
    "        #     print(instruction)\n",
    "        #     continue\n",
    "        break\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format=\"{Response: response}\"\n",
    "\n",
    "def generate_response(prompt, response_format=response_format):\n",
    "    cnt=0\n",
    "    output = None\n",
    "    while cnt!=2:\n",
    "        cnt+=1\n",
    "        # output = get_gpt(prompt,response_format)\n",
    "        output = get_vllm(prompt,response_schema)\n",
    "        break\n",
    "        # try:\n",
    "        #     # output = json.loads(output)[\"Response\"]\n",
    "        #     output = output[\"Response\"]\n",
    "        #     break\n",
    "        # except:\n",
    "        #     print(output)\n",
    "        #     # time.sleep(1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Instruction': 'Write a function in Python to count the number of vowels in a given string.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tmp = instruction_prompt.format(*instructions[0])\n",
    "generate_instruction(tmp,instruction_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_format = \"\"\"You are asked to generate the response according to the instruction. Here are some examples:\n",
    "# Example 1: \n",
    "#     Instruction: {}\n",
    "#     Response: {}\n",
    "# Example 2: \n",
    "#     Instruction: {}\n",
    "#     Response: {}\n",
    "\n",
    "# Generate the response of the following instruction: {}\n",
    "# \"\"\"\n",
    "response_format = \"\"\"Instruction: {}\n",
    "Response: {}\n",
    "Instruction: {}\n",
    "Response: {}\n",
    "Instruction: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response_prompt(idx, instruction):\n",
    "    response_seqs = []\n",
    "    for data in response_examples[idx]:\n",
    "        response_seqs.extend([data[\"instruction\"],data[\"response\"]])\n",
    "    response_seqs.append(instruction)\n",
    "    prompt = response_format.format(*response_seqs)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write an algorithm that prints the first 10 Fibonacci numbers in descending order.', '# Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)', 'Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30', 'UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;']\n"
     ]
    }
   ],
   "source": [
    "response_seqs = []\n",
    "for data in response_examples[0]:\n",
    "    response_seqs.extend([data[\"instruction\"],data[\"response\"]])\n",
    "print(response_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample 1: \\n    Instruction: Write an algorithm that prints the first 10 Fibonacci numbers in descending order.\\n    Response: # Initialize the first two numbers\\na = 0\\nb = 1\\n\\n# Print the first 10 Fibonacci numbers\\nfor i in range(10):\\n    # Swap the values\\n    a, b = b, a + b\\n    # Print the current Fibonacci number\\n    print(b)\\n# Reverse the list\\nfor _ in range(9, 0, -1):\\n    print(_)\\nExample 2: \\n    Instruction: Write a SQL statement to update the age of a customer with a specific ID. Customer_id = 24, Age = 30\\n    Response: UPDATE customers \\nSET age = 30\\nWHERE customer_id = 24;\\n\\nGenerate the response of this instruction, Instruction: Write a Python function that takes a string as input and returns the string with all vowels removed.\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a Python function that takes a string as input and returns the string with all vowels removed.\"\n",
    "response_seqs.append(instruction)\n",
    "tmp_prompt = response_format.format(*response_seqs)\n",
    "tmp_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def remove_vowels(input_string):\\\\\\\\n    vowels = 'aeiouAEIOU'\\\\\\\\n    output_string = ''\\\\\\\\n    for char in input_string:\\\\\\\\n        if char not in vowels:\\\\\\\\n            output_string += char\\\\n    return output_string\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(tmp_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:01<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "gen_instructions = []\n",
    "gen_responses = []\n",
    "for i in tqdm(range(50)):\n",
    "    tmp = instruction_prompt.format(*instructions[i])\n",
    "    instruction = generate_instruction(tmp,instruction_format)\n",
    "    prompt = format_response_prompt(i,instruction)\n",
    "    response = generate_response(prompt)\n",
    "    gen_instructions.append(instruction)\n",
    "    gen_responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_instruction_prompt = [instruction_prompt.format(*instruction) for instruction in instructions]\n",
    "formatted_instruction_prompt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_instructions = generate_instruction(formatted_instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"instruction\": gen_instructions,\n",
    "    \"response\": gen_responses,\n",
    "})\n",
    "df.to_csv(f\"./code.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æœˆä¹‹æš—é¢ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Instruction': 'Write a Python function that calculates the factorial of a non-negative integer. The function should handle input validation to ensure the provided argument is an integer and is not negative, and it should return the factorial of the number if valid or an appropriate error message otherwise.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    api_key=\"sk-Nl2yxo0Ic9ZJ0zFf3qWdypuoQ6Nd2xnsCp7wfojlW9OyOYmq\", # åœ¨è¿™é‡Œå°† MOONSHOT_API_KEY æ›¿æ¢ä¸ºä½ ä»Ž Kimi å¼€æ”¾å¹³å°ç”³è¯·çš„ API Key\n",
    "    base_url=\"https://api.moonshot.cn/v1\",\n",
    ")\n",
    "\n",
    "# {\n",
    "#     \"response\": \"text\",\n",
    "# }\n",
    "prompt = \"\"\"You are asked to come up with instructions. These instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions. Don't repeat instructions in examples. Here are some examples: \n",
    "Instruction 1: Write a for loop in JavaScript that prints numbers from 0 to 5. \n",
    "Instruction 2: Compare two objects and return 0 if they are equal, -1 if the first is less than the second, and 1 if the first is greater than the second. object1 = 7 object2 = 8 \n",
    "Instruction 3: Develop a code to sort a list in ascending or descending order. givenList: [5, -3, 2, 4, 0], sortOrder: \"ascending\" \n",
    "Instruction 4: Insert a node at the beginning of a linked list. [10 -> 15 -> 20 -> 25] \n",
    "Provide a new instruction below.\"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the intelligent customer service of Dark Side of the Moon (Kimi).\n",
    "Please use the following JSON format to output your response:\n",
    "{\"Instruction\": \"instruction\",}\n",
    "\"\"\"\n",
    " \n",
    "completion = client.chat.completions.create(\n",
    "    model=\"moonshot-v1-8k\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, # <-- å°†é™„å¸¦è¾“å‡ºæ ¼å¼çš„ system prompt æäº¤ç»™ Kimi\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    frequency_penalty=1.0,\n",
    "    response_format={\"type\": \"json_object\"}, # <-- ä½¿ç”¨ response_format å‚æ•°æŒ‡å®šè¾“å‡ºæ ¼å¼ä¸º json_object\n",
    ")\n",
    " \n",
    "content = json.loads(completion.choices[0].message.content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75772\n"
     ]
    }
   ],
   "source": [
    "data_concated: Dataset = processed_data[0]\n",
    "random.seed(42)\n",
    "iid_idxs = random.sample(range(len(data_concated)), 1000)\n",
    "base_data = data_concated.select(iid_idxs)\n",
    "clients_data = []\n",
    "for i in range(10):\n",
    "    clients_data.append(base_data.shard(10,i))\n",
    "\n",
    "data_concated = data_concated.select(list(set(range(len(data_concated)))-set(iid_idxs)))\n",
    "print(len(data_concated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'response', 'label']\n"
     ]
    }
   ],
   "source": [
    "print(clients_data[0].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸º output.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "domain = \"financial\"\n",
    "# å®šä¹‰è½¬æ¢å‡½æ•°ï¼Œå°†æ•°æ®é›†ä¸­çš„ä¸€è¡Œè½¬æ¢ä¸ºæ‰€éœ€çš„JSONLæ ¼å¼\n",
    "def convert_to_jsonl(i,row):\n",
    "    return {\n",
    "        \"id\": f\"seed_task_{i}\",\n",
    "        \"name\": row['name'] if 'name' in row else domain,  # å‡è®¾æ¯è¡Œéƒ½æœ‰'name'åˆ—ï¼Œå¦‚æžœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼\n",
    "        \"instruction\": row['instruction'],\n",
    "        \"instances\": [{\"input\": \"\", \"output\": row['response']}],\n",
    "        \"is_classification\": False\n",
    "    }\n",
    "\n",
    "# æ‰“å¼€æ–‡ä»¶ç”¨äºŽå†™å…¥\n",
    "with open('output.jsonl', 'w') as f:\n",
    "    # éåŽ†æ•°æ®é›†ä¸­çš„æ‰€æœ‰è¡Œ\n",
    "    for i,item in enumerate(clients_data[0]):  # å‡è®¾æˆ‘ä»¬å¤„ç†çš„æ˜¯è®­ç»ƒé›†\n",
    "        # è½¬æ¢ä¸ºJSONLæ ¼å¼\n",
    "        jsonl_line = convert_to_jsonl(i,item)\n",
    "        # å°†JSONå¯¹è±¡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œå¹¶å†™å…¥æ–‡ä»¶\n",
    "        f.write(json.dumps(jsonl_line) + '\\n')\n",
    "\n",
    "print(\"è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜ä¸º output.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm async api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_formatted = [instruction_prompt.format(*instruction) for instruction in instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import aiohttp\n",
    "import huggingface_hub.constants\n",
    "from tqdm.asyncio import tqdm\n",
    "from transformers import (AutoTokenizer, PreTrainedTokenizer,\n",
    "                          PreTrainedTokenizerFast,PreTrainedTokenizerBase)\n",
    "from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple\n",
    "import asyncio\n",
    "\n",
    "\n",
    "AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_request(\n",
    "    input_requests: List[Tuple[str, int, int]],\n",
    "    request_rate: float,\n",
    ") -> AsyncGenerator[Tuple[str, int, int], None]:\n",
    "    input_requests = iter(input_requests)\n",
    "    for request in input_requests:\n",
    "        yield request\n",
    "\n",
    "        if request_rate == float(\"inf\"):\n",
    "            # If the request rate is infinity, then we don't need to wait.\n",
    "            continue\n",
    "\n",
    "        # Sample the request interval from the exponential distribution.\n",
    "        interval = np.random.exponential(1.0 / request_rate)\n",
    "        # The next request will be sent after the interval.\n",
    "        await asyncio.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestFuncInput:\n",
    "    prompt: str\n",
    "    api_url: str\n",
    "    model: str\n",
    "    best_of: int = 1\n",
    "    use_beam_search: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RequestFuncOutput:\n",
    "    generated_text: str = \"\"\n",
    "    success: bool = False\n",
    "    latency: float = 0.0\n",
    "    ttft: float = 0.0  # Time to first token\n",
    "    itl: List[float] = field(\n",
    "        default_factory=list)  # List of inter-token latencies\n",
    "    error: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_schema = \"\"\"{\n",
    "    \"title\": \"Instruction Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Instruction\": {\n",
    "            \"title\": \"instruction\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Instruction\"]\n",
    "}\"\"\"\n",
    "\n",
    "response_schema = \"\"\"{\n",
    "    \"title\": \"Response Schema\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Response\": {\n",
    "            \"title\": \"response\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"Response\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text: str, prefix: str) -> str:\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "    \n",
    "async def async_request_openai_completions(\n",
    "    request_func_input: RequestFuncInput,\n",
    "    format: str,\n",
    "    pbar: Optional[tqdm] = None,\n",
    ") -> RequestFuncOutput:\n",
    "    api_url = request_func_input.api_url\n",
    "    assert api_url.endswith(\n",
    "        (\"completions\", \"profile\")\n",
    "    ), \"OpenAI Completions API URL must end with 'completions' or 'profile'.\"\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:\n",
    "        assert not request_func_input.use_beam_search\n",
    "        payload = {\n",
    "            \"model\": request_func_input.model,\n",
    "            \"prompt\": json.dumps(\n",
    "                [\n",
    "                    # {\"role\": \"system\", \"content\": f\"You are a helpful assistant designed to generate response to the instruction in JSON\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{request_func_input.prompt}\"}\n",
    "                ]),\n",
    "            \"temperature\": 0.7,\n",
    "            \"best_of\": request_func_input.best_of,\n",
    "            \"max_tokens\": 512,\n",
    "            \"min_tokens\": 50,\n",
    "            # \"guided_json\": format,\n",
    "            \"stream\": True,\n",
    "            \"frequency_penalty\": 1.0,\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\"\n",
    "        }\n",
    "\n",
    "        output = RequestFuncOutput()\n",
    "\n",
    "        generated_text = \"\"\n",
    "        ttft = 0.0\n",
    "        st = time.perf_counter()\n",
    "        most_recent_timestamp = st\n",
    "        try:\n",
    "            async with session.post(url=api_url, json=payload,\n",
    "                                    headers=headers) as response:\n",
    "                if response.status == 200:\n",
    "                    async for chunk_bytes in response.content:\n",
    "                        chunk_bytes = chunk_bytes.strip()\n",
    "                        if not chunk_bytes:\n",
    "                            continue\n",
    "\n",
    "                        chunk = remove_prefix(chunk_bytes.decode(\"utf-8\"),\n",
    "                                              \"data: \")\n",
    "                        if chunk == \"[DONE]\":\n",
    "                            latency = time.perf_counter() - st\n",
    "                        else:\n",
    "                            data = json.loads(chunk)\n",
    "\n",
    "                            # NOTE: Some completion API might have a last\n",
    "                            # usage summary response without a token so we\n",
    "                            # want to check a token was generated\n",
    "                            if data[\"choices\"][0][\"text\"]:\n",
    "                                timestamp = time.perf_counter()\n",
    "                                # First token\n",
    "                                if ttft == 0.0:\n",
    "                                    ttft = time.perf_counter() - st\n",
    "                                    output.ttft = ttft\n",
    "\n",
    "                                # Decoding phase\n",
    "                                else:\n",
    "                                    output.itl.append(timestamp -\n",
    "                                                      most_recent_timestamp)\n",
    "\n",
    "                                most_recent_timestamp = timestamp\n",
    "                                generated_text += data[\"choices\"][0][\"text\"]\n",
    "\n",
    "                    output.generated_text = generated_text\n",
    "                    output.success = True\n",
    "                    output.latency = latency\n",
    "                else:\n",
    "                    output.error = response.reason or \"\"\n",
    "                    output.success = False\n",
    "        except Exception:\n",
    "            output.success = False\n",
    "            exc_info = sys.exc_info()\n",
    "            output.error = \"\".join(traceback.format_exception(*exc_info))\n",
    "\n",
    "    if pbar:\n",
    "        pbar.update(1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def vllm_run(\n",
    "    api_url: str,\n",
    "    model_id: str,\n",
    "    input_requests: List[Tuple[str, int, int]],\n",
    "    best_of: int,\n",
    "    use_beam_search: bool,\n",
    "    request_rate: float,\n",
    "    disable_tqdm: bool,\n",
    "    profile: bool,\n",
    "    format: str,\n",
    "    ):\n",
    "    pbar = None if disable_tqdm else tqdm(total=len(input_requests))\n",
    "    tasks: List[asyncio.Task] = []\n",
    "    async for request in get_request(input_requests, request_rate):\n",
    "        prompt = request\n",
    "        request_func_input = RequestFuncInput(\n",
    "            model=model_id,\n",
    "            prompt=prompt,\n",
    "            api_url=api_url,\n",
    "            best_of=best_of,\n",
    "            use_beam_search=use_beam_search,\n",
    "        )\n",
    "        tasks.append(\n",
    "            asyncio.create_task(\n",
    "                async_request_openai_completions(request_func_input=request_func_input,format=format,\n",
    "                                pbar=pbar)))\n",
    "    outputs: List[RequestFuncOutput] = await asyncio.gather(*tasks)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_result = asyncio.run(\n",
    "    vllm_run(\n",
    "        api_url=f\"http://localhost:8000/v1/completions\",\n",
    "        model_id=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "        input_requests=instructions_formatted,\n",
    "        best_of=1,\n",
    "        use_beam_search=False,\n",
    "        request_rate=float(\"inf\"),\n",
    "        disable_tqdm=False,\n",
    "        profile=False,\n",
    "        format=instruction_schema\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [json.loads(res.generated_text)[\"Instruction\"] for res in benchmark_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Write a function that takes a string and returns the first 5 characters of that string. If the string is less than 5 characters, return the entire string.',\n",
       " 'Write a Python script to calculate the mean of the grades of students in a class. The grades are given as input by the user and should be separated by spaces.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_response_prompt = [format_response_prompt(i, instruction) for i,instruction in zip(range(100),result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_response_prompt[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark_result = asyncio.run(\n",
    "    vllm_run(\n",
    "        api_url=f\"http://localhost:8001/v1/completions\",\n",
    "        # model_id=\"allenai/open-instruct-code-alpaca-7b\",\n",
    "        model_id=\"Meta-Llama-3.1-8B-Instruct\",\n",
    "        input_requests=formatted_response_prompt,\n",
    "        best_of=1,\n",
    "        use_beam_search=False,\n",
    "        request_rate=float(\"inf\"),\n",
    "        disable_tqdm=False,\n",
    "        profile=False,\n",
    "        format=response_schema\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_result = [res.generated_text for res in benchmark_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [{\"role\": \"assistant\", \"content\": \"Here is the Python code that prints binary tree elements level by level: \\\\n\\\\nclass BinaryTree: \\\\n    def __init__(self, root): \\\\n        self.root = Node(root) \\\\n\\\\n    def printLevelOrder(self): \\\\n        h = self.height(self.root) \\\\n        for i in range(1, h+1): # to handle odd number of nodes\\\\n            self.printGivenLevel(self.root, i)  # <--- modified function call\\\\n  \\\\\\\\\\\\ndef height(self,node):\\\\ n\\\\tif node is None:\\\\ n\\\\t\\\\treturn 0\\\\ n\\\\telse:\\\\ n\\\\t\\\\treturn max(self.height(node.left),self.height(node.right))\\\\ndef printGivenLevel(root , level):\\\\ n   if (root == None):\\\\ n       return\\\\ n   if (level == 1):\\\\ ndb.printf(\\\\\"%d \\\\\" , root.data)\\\\nelse if (level > 1):\\\\ ndb.printGivenLevel(root.left , level-1)\\\\ndb.printGivenLevel(root.right , level-1)\\\\ndef Node(data): return TreeNode(data)\"}, {\"role\": \"assistant\", \"content\": \"Here is the Python code that represents a student with attributes and a method to get student information in a formatted output: \"}]], [\"print_level_order_result\", \"print_given_level_result\", \"\\\\u4e00\\\\u4e09\\\\u5341\\\\u767e\\\\u7684student.\", \"\\\\u5f53\\\\u524d\\\\u79cd\"] [{\"line_number\": \"\", \"code\": \"// A utility function to print all nodes at a given level \", \"match_type\": \"\", \"explanation\":\"\"}, {\"line_number\":\"\",\"code\":\"\",\"match_type\":\"\",\"explanation\":\"\"}, {\"line_number\":\"\",\"code\":\"// A utility function to print all nodes at a given level \",\"match_type\":\"\",\"explanation\":\"\"}, {\"line_number\":\"\",\"code\":\"void printGivenLevel (struct node* root , int level)\",\"match_type\":\"function_definition\",\"explanation\":\"\"}] [{\"numbered_line_english_name_with_space_after_line_numbering_and_code_contextual_explanations_only_for_first_match_of_each_function_definition_though_not_here_as_function_definitions_dont_have_lines_in_c_onlyfunctions_returns_false_in_python:_6__linenumberedlinename_withspaceafterline_numbers_foreachfunctiondefinitiononly_gives_error_for_c_code_here\"],\"function_name_to_print_tree_elements_by_level_without_matches_here_due_to_test_case_and_instruction_changes_from_printing_binary_tree'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"instruction\": result,\n",
    "    \"response\": res_result\n",
    "})\n",
    "df.to_csv(\"code.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
