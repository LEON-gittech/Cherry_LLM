{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        print(f\"Elapsed time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]\n",
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example\n",
    "\n",
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=2023)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset\n",
    "processed_data = []\n",
    "# for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\",\"tatsu-lab/alpaca\",\"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,general_data,math_data]):\n",
    "for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\", \"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,math_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad4c596552c4fb497f7c398b5024c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/224567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOMAIN_LABELS = {\n",
    "    'code': 0,\n",
    "    'finance': 1,\n",
    "    'medical': 2,\n",
    "    'math': 3\n",
    "}\n",
    "\n",
    "for i, name in zip(range(4),[\"code\",\"finance\",\"medical\",\"math\"]):\n",
    "    processed_data[i] = processed_data[i].add_column('domain', [DOMAIN_LABELS[name]] * len(processed_data[i]))\n",
    "\n",
    "# Concatenate the datasets\n",
    "concated_data = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': ['A math teacher has 24 cards, each of which is in the shape of a geometric figure. Half of the cards are rectangles, and a third of the cards are rhombuses. If 8 cards are squares, what is the maximum possible number of cards that re circles.\\nAnswer Choices: (A) 7 (B) 10 (C) 11 (D) 12 (E) 13', '2, 3, 6, 0, 10, -3, 14, (...)\\nAnswer Choices: (A) 6 (B) 2 (C) -2 (D) 0 (E) -6', \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. `` We have tailored our solutions to meet Solel 's technical requirements , and the result is both cost-effective manufacturing and highest-quality reflectors . ''\", 'Set an environment variable in Bash that holds the value of the given string. MyPassword', 'In a statistics class, a professor gives a midterm exam with a maximum score of 100. The scores received by a group of 20 students are:\\n\\n60, 70, 75, 80, 82, 84, 86, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 100, 100\\n\\nUsing the rule of thumb method for outlier detection, determine if any scores are outliers.'], 'response': [\"Let's reason about the multi-choice question.\\na square is a special kind of rhombus (sides are perpendicular)\\na square is a special kind of rectangles (sides with same length)\\nAmong the 24 cards with have:\\n15 rectangles\\n10 rhombus\\n8 squares\\nAmong the 15 rectangles, there could be 8 special ones (with sides of same length) that are squares. That lets at least 7 rectangles that are not square.\\nAmong the 10 rectangles, there could be 8 special ones (with sides perpendicular) that are squares. That lets at least 2 rhombus that are not square.\\nWe have 8 squares.\\nSo the minimum different cards that represent a square, a rhombus or a rectangle is 2 + 7 + 8 = 17\\nWhich means that the maximum number of circles that you could have is 24 - 17 = 7\\nThe answer is A\", \"Let's solve the multi-choice question step by step.\\nThere are two series\\n2, 6, 10, 14, ... (Adding 4)\\n3, 0, -3, ... (Subtracting 3)\\nHence, next term is -3 - 3 = -6\\nThe answer is E.\", 'positive', \"export MY_PASSWORD='MyPassword'\", 'To determine if there are any outliers using the rule of thumb method, we first need to calculate the interquartile range (IQR) and then use it to find the lower and upper bounds for the scores.\\n\\nStep 1: Calculate the quartiles.\\nQ1 (first quartile) is the median of the lower half of the data. In this case, the lower half is 60, 70, 75, 80, 82, 84, 86, 88, 90, 92. The median is the average of 82 and 84, so Q1 = 83.\\nQ3 (third quartile) is the median of the upper half of the data. In this case, the upper half is 93, 94, 95, 96, 97, 98, 99, 100, 100, 100. The median is the average of 96 and 97, so Q3 = 96.5.\\n\\nStep 2: Calculate the interquartile range (IQR).\\nIQR = Q3 - Q1 = 96.5 - 83 = 13.5\\n\\nStep 3: Calculate the lower and upper bounds.\\nLower bound = Q1 - 1.5 * IQR = 83 - 1.5 * 13.5 = 63.25\\nUpper bound = Q3 + 1.5 * IQR = 96.5 + 1.5 * 13.5 = 116.25\\n\\nStep 4: Check for outliers.\\nNow we check if any scores are below the lower bound or above the upper bound. In this case, all scores are between 63.25 and 116.25, so there are no outliers.'], 'domain': [3, 3, 1, 0, 3], '__index_level_0__': [116106, 198703, None, None, 252126]}\n"
     ]
    }
   ],
   "source": [
    "concated_data = concated_data.shuffle()\n",
    "print(concated_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5', torch_dtype=torch.bfloat16).cuda()\n",
    "\n",
    "def get_embedding(input_text, model):\n",
    "    encoded_input = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    model_output = model(**encoded_input)\n",
    "    instruction_embeddings = model_output[0][:, 0] # CLS token pooling\n",
    "    instruction_embeddings = torch.nn.functional.normalize(instruction_embeddings, p=2, dim=1)\n",
    "    return instruction_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.instructions = data['instruction']\n",
    "        self.domains = data['domain']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instructions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'instruction': self.instructions[idx],\n",
    "            'domain': self.domains[idx]\n",
    "        }\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        instructions = [f[\"instruction\"] for f in features]\n",
    "        domains = [f[\"domain\"] for f in features]\n",
    "        batch = self.tokenizer(instructions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        batch[\"domain\"] = torch.tensor(domains)\n",
    "        return batch\n",
    "\n",
    "instruction_dataset = InstructionDataset(concated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(anchor, positive, margin=1.0):\n",
    "    return F.relu(margin - F.cosine_similarity(anchor, positive)).mean()\n",
    "\n",
    "def supervised_contrastive_loss(embed, target, margin=1.0):\n",
    "    loss = 0.0\n",
    "    for i in range(len(embed)):\n",
    "        positive = torch.stack([embed[j] for j in range(len(embed)) if target[j] == target[i]])\n",
    "        negative = torch.stack([embed[j] for j in range(len(embed)) if target[j] != target[i]])\n",
    "        if positive.size(0) > 1:  # Ensure at least one positive example (excluding self)\n",
    "            anchor = embed[i].unsqueeze(0).repeat(positive.size(0), 1)\n",
    "            loss += contrastive_loss(anchor, positive, margin)\n",
    "            anchor = embed[i].unsqueeze(0).repeat(negative.size(0), 1)\n",
    "            loss += contrastive_loss(anchor, negative, margin)\n",
    "    return loss / len(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        instructions = inputs['input_ids']\n",
    "        domains = inputs['domain']\n",
    "        embeddings = model(input_ids=instructions, attention_mask=inputs['attention_mask'])[0][:, 0]\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return supervised_contrastive_loss(embeddings, domains)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    report_to=\"none\",\n",
    "    bf16=True,\n",
    "    eval_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    save_steps=1000,\n",
    "    save_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "collator = CustomDataCollator(tokenizer)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=instruction_dataset,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
