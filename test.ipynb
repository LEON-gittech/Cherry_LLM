{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.3.1+cu121)\n",
      "    Python  3.9.18 (you have 3.9.2)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.6\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.22.post7. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.6 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/mnt/bn/data-tns-live-llm/leon/datasets/repetition/cherry-alpaca-3\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"generate some instruction tuning sample used for your training\", return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.cuda()\n",
    "generate_ids = model.generate(input_ids, max_length=2048, repetition_penalty=1.1, streamer = text_streamer)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971bc59b19446548b9f15eac635d241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_cherry_1000.pt:   0%|          | 0.00/3.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467d4da32a541508c0ab3f45dff57c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_MoDS_1000.pt:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48502f13dc02456fb54242feded4e601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MathInstruct.json:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff2d3f044e24fdebad3cb6a8fb1c860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cherry_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375e8a0e273e497d9e4ffd4f984e4648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4981155df44538a02f26afc356b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MoDS_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a89fe838e420197ae2f8989ecf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_data_cherry.pt:   0%|          | 0.00/25.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c30c215484a5c8bd79a68aba4f7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "math_data_cherry.pt:   0%|          | 0.00/38.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/LEON24/files/commit/6463c78cb88699b514be76499756d33d9c6bcc64', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6463c78cb88699b514be76499756d33d9c6bcc64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "repo_id = \"LEON24/files\"\n",
    "folder_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/files\"\n",
    "api.create_repo(repo_id, exist_ok=True, private=True)\n",
    "api.upload_folder(\n",
    "    folder_path=folder_path,\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, T5ForConditionalGeneration\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-xl-lm-adapt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', '\"steps\"', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\n"
     ]
    }
   ],
   "source": [
    "args = \"\"\"--model_name_or_path google/t5-xl-lm-adapt --data_path /mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet --per_device_train_batch_size 256 --eval_strategy no --num_train_epochs 3 --learning_rate 2e-5 --warmup_ratio 0.03 --output_dir /mnt/bn/data-tns-live-llm/leon datasets/p3_exp1 --logging_steps 100 --save_strategy \"steps\" --save_steps 100 --save_total_limit 3 --model_max_length 1024\"\"\".split(\" \")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"--model_name_or_path\", \"google/t5-xl-lm-adapt\", \"--data_path\", \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet\", \"--per_device_train_batch_size\", \"256\", \"--eval_strategy\", \"no\", \"--num_train_epochs\", \"3\", \"--learning_rate\", \"2e-5\", \"--warmup_ratio\", \"0.03\", \"--output_dir\", \"/mnt/bn/data-tns-live-llm/leon\", \"datasets/p3_exp1\", \"--logging_steps\", \"100\", \"--save_strategy\", \"steps\", \"--save_steps\", \"100\", \"--save_total_limit\", \"3\", \"--model_max_length\", \"1024\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\".replace(\"\\'\",\"\\\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1/checkpoint-6/pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/t5-xl-lm-adapt\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m      8\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mprepare(model)\n\u001b[0;32m----> 9\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1/checkpoint-6/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# signature = inspect.signature(model.forward)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# signature_columns = list(signature.parameters.keys())\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(signature_columns)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/accelerator.py:3111\u001b[0m, in \u001b[0;36mAccelerator.load_state\u001b[0;34m(self, input_dir, **load_model_func_kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3109\u001b[0m         map_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 3111\u001b[0m load_accelerator_state(\n\u001b[1;32m   3112\u001b[0m     input_dir,\n\u001b[1;32m   3113\u001b[0m     models,\n\u001b[1;32m   3114\u001b[0m     optimizers,\n\u001b[1;32m   3115\u001b[0m     schedulers,\n\u001b[1;32m   3116\u001b[0m     dataloaders,\n\u001b[1;32m   3117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mprocess_index,\n\u001b[1;32m   3118\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler,\n\u001b[1;32m   3119\u001b[0m     map_location,\n\u001b[1;32m   3120\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mload_model_func_kwargs,\n\u001b[1;32m   3121\u001b[0m )\n\u001b[1;32m   3122\u001b[0m custom_checkpoints \u001b[39m=\u001b[39m [\n\u001b[1;32m   3123\u001b[0m     f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(input_dir) \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^custom_checkpoint_\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.pkl$\u001b[39m\u001b[39m\"\u001b[39m, f) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3124\u001b[0m ]\n\u001b[1;32m   3125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(custom_checkpoints) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_custom_objects):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/checkpointing.py:203\u001b[0m, in \u001b[0;36mload_accelerator_state\u001b[0;34m(input_dir, models, optimizers, schedulers, dataloaders, process_index, scaler, map_location, **load_model_func_kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[39m# Load with torch\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         input_model_file \u001b[39m=\u001b[39m input_dir\u001b[39m.\u001b[39mjoinpath(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mMODEL_NAME\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mending\u001b[39m}\u001b[39;00m\u001b[39m.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(input_model_file, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    204\u001b[0m     models[i]\u001b[39m.\u001b[39mload_state_dict(state_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mload_model_func_kwargs)\n\u001b[1;32m    205\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAll model weights loaded successfully\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1/checkpoint-6/pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, T5ForConditionalGeneration\n",
    "import torch\n",
    "import inspect\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "# config = AutoConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", torch_dtype=torch.bfloat16)\n",
    "accelerator.prepare(model)\n",
    "accelerator.load_state(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1/checkpoint-6/\")\n",
    "# signature = inspect.signature(model.forward)\n",
    "# signature_columns = list(signature.parameters.keys())\n",
    "# print(signature_columns)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
