{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.8\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_med_0.01_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model,\"/mnt/bn/data-tns-live-llm/leon/datasets/pos\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> generate some instruction tuning sample used for your training. —Å–∞–≤–µ–∑–Ω–∏ —ò–∞–≤–Ω–∏ —Ä–∞–¥–∏–æ –∏ —Ç–µ–ª–µ–≤–∏–∑–∏—ò–∞ 2014.\n",
      "generate some instruction tuning sample used for your training.</s>\n",
      "generate some instruction tuning sample used for your training. —Å–∞–≤–µ–∑–Ω–∏ —ò–∞–≤–Ω–∏ —Ä–∞–¥–∏–æ –∏ —Ç–µ–ª–µ–≤–∏–∑–∏—ò–∞ 2014.\n",
      "generate some instruction tuning sample used for your training.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"generate some instruction tuning sample used for your training\", return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.cuda()\n",
    "generate_ids = model.generate(input_ids, max_length=2048, repetition_penalty=1.1, streamer = text_streamer)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‰∏ä‰º†Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971bc59b19446548b9f15eac635d241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_cherry_1000.pt:   0%|          | 0.00/3.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467d4da32a541508c0ab3f45dff57c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_MoDS_1000.pt:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48502f13dc02456fb54242feded4e601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MathInstruct.json:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff2d3f044e24fdebad3cb6a8fb1c860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cherry_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375e8a0e273e497d9e4ffd4f984e4648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4981155df44538a02f26afc356b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MoDS_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a89fe838e420197ae2f8989ecf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_data_cherry.pt:   0%|          | 0.00/25.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c30c215484a5c8bd79a68aba4f7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "math_data_cherry.pt:   0%|          | 0.00/38.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/LEON24/files/commit/6463c78cb88699b514be76499756d33d9c6bcc64', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6463c78cb88699b514be76499756d33d9c6bcc64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "repo_id = \"LEON24/files\"\n",
    "folder_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/files\"\n",
    "api.create_repo(repo_id, exist_ok=True, private=True)\n",
    "api.upload_folder(\n",
    "    folder_path=folder_path,\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, T5ForConditionalGeneration\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-xl-lm-adapt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ë∞ÉËØïÁî®ÂèÇÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', '\"steps\"', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\n"
     ]
    }
   ],
   "source": [
    "args = \"\"\"--model_name_or_path google/t5-xl-lm-adapt --data_path /mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet --per_device_train_batch_size 256 --eval_strategy no --num_train_epochs 3 --learning_rate 2e-5 --warmup_ratio 0.03 --output_dir /mnt/bn/data-tns-live-llm/leon datasets/p3_exp1 --logging_steps 100 --save_strategy \"steps\" --save_steps 100 --save_total_limit 3 --model_max_length 1024\"\"\".split(\" \")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"--model_name_or_path\", \"google/t5-xl-lm-adapt\", \"--data_path\", \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet\", \"--per_device_train_batch_size\", \"256\", \"--eval_strategy\", \"no\", \"--num_train_epochs\", \"3\", \"--learning_rate\", \"2e-5\", \"--warmup_ratio\", \"0.03\", \"--output_dir\", \"/mnt/bn/data-tns-live-llm/leon\", \"datasets/p3_exp1\", \"--logging_steps\", \"100\", \"--save_strategy\", \"steps\", \"--save_steps\", \"100\", \"--save_total_limit\", \"3\", \"--model_max_length\", \"1024\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\".replace(\"\\'\",\"\\\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂêàÂπ∂Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: InvalidHeaderDeserialization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_path)\n\u001b[0;32m---> 13\u001b[0m model:PeftModel \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model = model.merge_and_unload()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model.save_pretrained(target_path)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# signature_columns = list(signature.parameters.keys())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(signature_columns)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/peft/peft_model.py:430\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     model \u001b[39m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[39m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 430\u001b[0m model\u001b[39m.\u001b[39;49mload_adapter(model_id, adapter_name, is_trainable\u001b[39m=\u001b[39;49mis_trainable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    431\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/peft/peft_model.py:984\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m         peft_config\u001b[39m.\u001b[39minference_mode \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m is_trainable\n\u001b[1;32m    982\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_adapter(adapter_name, peft_config)\n\u001b[0;32m--> 984\u001b[0m adapters_weights \u001b[39m=\u001b[39m load_peft_weights(model_id, device\u001b[39m=\u001b[39;49mtorch_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhf_hub_download_kwargs)\n\u001b[1;32m    986\u001b[0m \u001b[39m# load the weights into the model\u001b[39;00m\n\u001b[1;32m    987\u001b[0m ignore_mismatched_sizes \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mignore_mismatched_sizes\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:444\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         adapters_weights \u001b[39m=\u001b[39m safe_load_file(filename, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         adapters_weights \u001b[39m=\u001b[39m safe_load_file(filename, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     adapters_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(filename, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/safetensors/torch.py:311\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39mLoads a safetensors file into torch format.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 311\u001b[0m \u001b[39mwith\u001b[39;00m safe_open(filename, framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    313\u001b[0m         result[k] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mget_tensor(k)\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: InvalidHeaderDeserialization"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, T5ForConditionalGeneration, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import inspect\n",
    "from accelerate import Accelerator\n",
    "from peft import PeftModel\n",
    "accelerator = Accelerator()\n",
    "base_path = \"google/t5-xl-lm-adapt\"\n",
    "source_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1\"\n",
    "target_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp2_merged/\"\n",
    "# config = AutoConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "model:PeftModel = PeftModel.from_pretrained(model, source_path)\n",
    "print(model)\n",
    "# model = model.merge_and_unload()\n",
    "# print(model)\n",
    "# model.save_pretrained(target_path)\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "# tokenizer.save_pretrained(target_path)\n",
    "# signature = inspect.signature(model.forward)\n",
    "# signature_columns = list(signature.parameters.keys())\n",
    "# print(signature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1_merged\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5ForConditionalGeneration(\n",
      "      (shared): Embedding(32128, 2048)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 2048)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 2048)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 18,874,368 || all params: 2,868,631,552 || trainable%: 0.6580\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", quantization_config=quantization_config)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-xl-lm-adapt\", model_max_length=1024)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=32,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\",\n",
    "        # target_modules= ['v', 'o'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): T5ForConditionalGeneration(\n",
      "      (shared): Embedding(32128, 2048)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 2048)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 2048)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (relative_attention_bias): Embedding(32, 32)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (k): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                  (v): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wi_1): Linear4bit(in_features=2048, out_features=5120, bias=False)\n",
      "                  (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp/tokenizer_config.json',\n",
       " '/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp/special_tokens_map.json',\n",
       " '/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp/spiece.model',\n",
       " '/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp/added_tokens.json',\n",
       " '/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp/tokenizer.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp\"\n",
    "model.merge_adapter()\n",
    "print(model)\n",
    "model.save_pretrained(target_path)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "tokenizer.save_pretrained(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_config = PeftConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_pos_public_20000_fedavg_c10s2_i10_b16a2_l2048_r16a16/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
