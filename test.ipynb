{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-24 00:12:52,376] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/mnt/bn/data-tns-live-llm/leon/datasets/llama-3-8b-bnb-4bit/\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "# model = PeftModel.from_pretrained(model,\"/mnt/bn/data-tns-live-llm/leon/datasets/pos\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d0aa04b4b04a26b006b6b4771b6f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /mnt/bn/data-tns-live-llm/leon/datasets/Meta-Llama-3-8B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForSequenceClassification, LlamaForTokenClassification\n",
    "model = LlamaForSequenceClassification.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/Meta-Llama-3-8B/\",torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"/mnt/bn/data-tns-live-llm/leon/datasets/Meta-Llama-3-8B/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 15339, 1917], 'attention_mask': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> generate some instruction tuning sample used for your training. —Å–∞–≤–µ–∑–Ω–∏ —ò–∞–≤–Ω–∏ —Ä–∞–¥–∏–æ –∏ —Ç–µ–ª–µ–≤–∏–∑–∏—ò–∞ 2014.\n",
      "generate some instruction tuning sample used for your training.</s>\n",
      "generate some instruction tuning sample used for your training. —Å–∞–≤–µ–∑–Ω–∏ —ò–∞–≤–Ω–∏ —Ä–∞–¥–∏–æ –∏ —Ç–µ–ª–µ–≤–∏–∑–∏—ò–∞ 2014.\n",
      "generate some instruction tuning sample used for your training.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"generate some instruction tuning sample used for your training\", return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.cuda()\n",
    "generate_ids = model.generate(input_ids, max_length=2048, repetition_penalty=1.1, streamer = text_streamer)\n",
    "outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‰∏ä‰º†Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3971bc59b19446548b9f15eac635d241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_cherry_1000.pt:   0%|          | 0.00/3.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467d4da32a541508c0ab3f45dff57c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_MoDS_1000.pt:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48502f13dc02456fb54242feded4e601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MathInstruct.json:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff2d3f044e24fdebad3cb6a8fb1c860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cherry_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375e8a0e273e497d9e4ffd4f984e4648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4981155df44538a02f26afc356b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MoDS_1000.pt:   0%|          | 0.00/3.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51a89fe838e420197ae2f8989ecf6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_data_cherry.pt:   0%|          | 0.00/25.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c30c215484a5c8bd79a68aba4f7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "math_data_cherry.pt:   0%|          | 0.00/38.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/LEON24/files/commit/6463c78cb88699b514be76499756d33d9c6bcc64', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6463c78cb88699b514be76499756d33d9c6bcc64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "repo_id = \"LEON24/files\"\n",
    "folder_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/files\"\n",
    "api.create_repo(repo_id, exist_ok=True, private=True)\n",
    "api.upload_folder(\n",
    "    folder_path=folder_path,\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel, T5ForConditionalGeneration\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-xl-lm-adapt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ë∞ÉËØïÁî®ÂèÇÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', '\"steps\"', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\n"
     ]
    }
   ],
   "source": [
    "args = \"\"\"--model_name_or_path google/t5-xl-lm-adapt --data_path /mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet --per_device_train_batch_size 256 --eval_strategy no --num_train_epochs 3 --learning_rate 2e-5 --warmup_ratio 0.03 --output_dir /mnt/bn/data-tns-live-llm/leon datasets/p3_exp1 --logging_steps 100 --save_strategy \"steps\" --save_steps 100 --save_total_limit 3 --model_max_length 1024\"\"\".split(\" \")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"--model_name_or_path\", \"google/t5-xl-lm-adapt\", \"--data_path\", \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet\", \"--per_device_train_batch_size\", \"256\", \"--eval_strategy\", \"no\", \"--num_train_epochs\", \"3\", \"--learning_rate\", \"2e-5\", \"--warmup_ratio\", \"0.03\", \"--output_dir\", \"/mnt/bn/data-tns-live-llm/leon\", \"datasets/p3_exp1\", \"--logging_steps\", \"100\", \"--save_strategy\", \"steps\", \"--save_steps\", \"100\", \"--save_total_limit\", \"3\", \"--model_max_length\", \"1024\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"['--model_name_or_path', 'google/t5-xl-lm-adapt', '--data_path', '/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1.parquet', '--per_device_train_batch_size', '256', '--eval_strategy', 'no', '--num_train_epochs', '3', '--learning_rate', '2e-5', '--warmup_ratio', '0.03', '--output_dir', '/mnt/bn/data-tns-live-llm/leon', 'datasets/p3_exp1', '--logging_steps', '100', '--save_strategy', 'steps', '--save_steps', '100', '--save_total_limit', '3', '--model_max_length', '1024']\".replace(\"\\'\",\"\\\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 2048)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 2048)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 32)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
      "              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂêàÂπ∂Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, T5ForConditionalGeneration, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import inspect\n",
    "from accelerate import Accelerator\n",
    "from peft import PeftModel\n",
    "accelerator = Accelerator()\n",
    "base_path = \"google/t5-xl-lm-adapt\"\n",
    "source_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1\"\n",
    "target_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp2_merged/\"\n",
    "# config = AutoConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_path, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "model:PeftModel = PeftModel.from_pretrained(model, source_path)\n",
    "print(model)\n",
    "# model = model.merge_and_unload()\n",
    "# print(model)\n",
    "# model.save_pretrained(target_path)\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "# tokenizer.save_pretrained(target_path)\n",
    "# signature = inspect.signature(model.forward)\n",
    "# signature_columns = list(signature.parameters.keys())\n",
    "# print(signature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/p3_exp1_merged\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\"google/t5-xl-lm-adapt\", quantization_config=quantization_config)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-xl-lm-adapt\", model_max_length=1024)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=32,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_2_SEQ_LM\",\n",
    "        # target_modules= ['v', 'o'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"/mnt/bn/data-tns-live-llm/leon/datasets/p3_tmp\"\n",
    "model.merge_adapter()\n",
    "print(model)\n",
    "model.save_pretrained(target_path)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "tokenizer.save_pretrained(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_config = PeftConfig.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_pos_public_20000_fedavg_c10s2_i10_b16a2_l2048_r16a16/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code generation with vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 13:18:51 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_scaffold_merged', speculative_config=None, tokenizer='/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_scaffold_merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_scaffold_merged, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-23 13:18:53 model_runner.py:720] Starting to load model /mnt/bn/data-tns-live-llm/leon/datasets/fed/code_scaffold_merged...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918e3181237148cb8c3c6d6029713ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-23 13:19:41 model_runner.py:732] Loading model weights took 14.9595 GB\n",
      "INFO 08-23 13:19:41 gpu_executor.py:102] # GPU blocks: 20150, # CPU blocks: 2048\n",
      "INFO 08-23 13:19:43 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-23 13:19:43 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-23 13:19:55 model_runner.py:1225] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "llm = LLM(model=\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_scaffold_merged\", tensor_parallel_size=1, \n",
    "    dtype=torch.bfloat16, trust_remote_code=True, \n",
    "    enable_lora=False, max_model_len=2048, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [\"USER:\", \"ASSISTANT:\",  \"### Instruction:\", \"Response:\", \n",
    "                \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\", \"<|eot_id|>\", \"####\"]\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=1, max_tokens=1024, repetition_penalty=1.1, stop=stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.65it/s, est. speed input: 197.18 toks/s, output: 76.22 toks/s]\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(input, sampling_params)[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    for i in range(len(numbers)):\\n        for j in range(i + 1, len(numbers)):\\n            dist = abs(numbers[i] - numbers[j])\\n            if dist < threshold:\\n                return True\\n\\n    return False'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, task_id in enumerate(problems):\n",
    "#     print(task_id)\n",
    "#     print(problems[task_id])\n",
    "#     if i==10:break\n",
    "print(args.model_name_or_path)\n",
    "num_samples_per_task = 1\n",
    "samples = [\n",
    "    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n",
    "    for task_id in problems\n",
    "    # for _ in range(num_samples_per_task)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
