{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造niid和iid的jsonl，看看其对应的广度和质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]\n",
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example\n",
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=2023)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset\n",
    "processed_data = []\n",
    "for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\",\"tatsu-lab/alpaca\",\"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,general_data,math_data]):\n",
    "# for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\", \"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,math_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)\n",
    "    \n",
    "data_concated = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 8*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"\",\n",
    "                  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_block(A,B,block_size=50000):\n",
    "    C = torch.zeros(A.size(0), B.size(1))\n",
    "    # 进行分块矩阵乘法\n",
    "    for i in range(0, A.size(0), block_size):\n",
    "        for j in range(0, B.size(1), block_size):\n",
    "            for k in range(0, A.size(1), block_size):\n",
    "                # 计算分块索引\n",
    "                i_end = min(i + block_size, A.size(0))\n",
    "                j_end = min(j + block_size, B.size(1))\n",
    "                k_end = min(k + block_size, A.size(1))\n",
    "                # 执行子块乘法并累加到结果矩阵中\n",
    "                C[i:i_end, j:j_end] += torch.mm(A[i:i_end, k:k_end].cuda(), B[k:k_end, j:j_end].cuda()).cpu()\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as cosine_similarity\n",
    "\n",
    "def coverage(A, V):\n",
    "    # 将输入转换为张量\n",
    "    A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "    V_tensor = torch.tensor(V, dtype=torch.float32)\n",
    "    # 计算集合A的覆盖广度\n",
    "    # similarities = matmul_block(V_tensor,A_tensor.T)\n",
    "    similarities = torch.matmul(V_tensor, A_tensor.T)\n",
    "    # 计算每个v的最大相似度\n",
    "    max_similarities = torch.max(similarities, dim=1).values\n",
    "    # 计算总相似度\n",
    "    total_similarity = torch.sum(max_similarities).item()/len(max_similarities)\n",
    "    return total_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 199/199 [03:02<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "data_concated_embeddings = model.encode(data_concated[\"instruction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## niid的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from scipy.spatial import ConvexHull\n",
    "from cuml.manifold import UMAP, TSNE\n",
    "\n",
    "def umap_embeddings_volume(embeddings):\n",
    "    # 初始化 UMAP 降维模型\n",
    "    reducer = umap.UMAP(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def cuml_tsne_embeddings_volume(embeddings):\n",
    "    reducer = TSNE(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def cuml_umap_embeddings_volume(embeddings):\n",
    "    reducer = UMAP(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算数据平均质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.265625\n"
     ]
    }
   ],
   "source": [
    "reward_name = \"/mnt/bn/data-tns-live-llm/leon/datasets/reward-model-deberta-v3-large-v2/\"\n",
    "device = \"cuda\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name, torch_dtype=torch.bfloat16).to(device), AutoTokenizer.from_pretrained(reward_name)\n",
    "# rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name, load_in_4bit=True), AutoTokenizer.from_pretrained(reward_name)\n",
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt').to(device)\n",
    "score = rank_model(**inputs).logits[0].detach()\n",
    "print(float(score))\n",
    "\n",
    "def quality_evaluation(datas):\n",
    "    score = 0\n",
    "    cnt = 0\n",
    "    result_list = []\n",
    "    for element in tqdm(datas):\n",
    "        instruction = element['instruction']\n",
    "        _input = ''\n",
    "        if 'input' in element.keys():\n",
    "            _input = element['input']\n",
    "        _output = element['response']\n",
    "        question = ''\n",
    "        if _input == '':\n",
    "            question = instruction\n",
    "        else:\n",
    "            question = instruction + '\\n' +_input\n",
    "        \n",
    "        answer = _output\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(question, answer, return_tensors='pt').to(device)\n",
    "            score += rank_model(**inputs).logits[0].detach()\n",
    "            cnt +=1\n",
    "        except:\n",
    "            print(instruction)\n",
    "            print(_output)\n",
    "            continue\n",
    "        \n",
    "    print(score/cnt)\n",
    "        # final_result = {'instruction':instruction,'input':_input,'response':_output,'reward_score':float(score)}\n",
    "        # result_list.append(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4219], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:30<00:00, 33.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4102], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 34.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5273], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 34.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4238], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "    quality_evaluation(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\", \"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "    quality_evaluation(datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用gpt评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Instruction: write a joke about biden Input: None Response: oh biden, sleepy joe!'}, {'role': 'user', 'content': 'Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "messages_template =[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Instruction: {instruction} Input: None Response: {response}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"\"\"\n",
    "    },\n",
    "    ]\n",
    "\n",
    "messages_template[0][\"content\"] = messages_template[0][\"content\"].format_map({\"instruction\":\"write a joke about biden\",\"response\":\"oh biden, sleepy joe!\"})\n",
    "print(messages_template)\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "data = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": messages_template,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 256,\n",
    "}\n",
    "headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "# data = {k: v for k, v in data.items() if v is not None}\n",
    "# data = json.dumps(data)\n",
    "# url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "# response = requests.post(url, data=data, headers=headers)\n",
    "# print(response.content)\n",
    "# score = int(response.json()[\"choices\"][0][\"message\"][\"content\"][0])\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- standard request -----\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "client = Ark(base_url=\"https://ark.cn-beijing.volces.com/api/v3\",api_key=\"\")\n",
    "\n",
    "# Non-streaming:\n",
    "print(\"----- standard request -----\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Instruction: Remove all the punctuation from a given string \"Welcome to the world of computers!\", Response: 'import string\\n\\ndef remove_punctuation(text):\\n    punctuations = string.punctuation\\n    no_punct = \"\"\\n    for char in text:\\n        if char not in punctuations:\\n            no_punct += char\\n    return no_punct\\n\\nif __name__ == \\'__main__\\':\\n    text = \\'Welcome to the world of computers!\\'\\n    print(remove_punctuation(text))'\"\"\"},\n",
    "    ],\n",
    "    # temperature=0.1,\n",
    "    # max_tokens=256\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def request_doubao(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ep-20240804150426-2vkvx\",\n",
    "        messages = messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    try:\n",
    "        response = response.choices[0].message.content\n",
    "        match = re.search(r'\\d+', response)\n",
    "        score = int(match.group(0))\n",
    "    except Exception as e: \n",
    "        print(\"request\",response)\n",
    "        score = 3\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "\n",
    "messages_template =[\n",
    "{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"\"\"\n",
    "},\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"Instruciton: {instruction} Response: {response}\"\"\"\n",
    "},\n",
    "]\n",
    "\n",
    "def request_gpt(messages):\n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 256,\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "    data = {k: v for k, v in data.items() if v is not None}\n",
    "    data = json.dumps(data)\n",
    "    url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "    # response = requests.post(url, data=data, headers=headers)\n",
    "    # if \"choices\" not in response.json().keys(): response = requests.post(url, data=data, headers=headers)\n",
    "    try: \n",
    "        response = requests.post(url, data=data, headers=headers)\n",
    "        response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        match = re.search(r'\\d+', response)\n",
    "        score = int(match.group(0))\n",
    "        # score = int(response.json()[\"choices\"][0][\"message\"][\"content\"][0])\n",
    "    except Exception as e: \n",
    "        print(\"request\",response)\n",
    "        score = 3\n",
    "    return score\n",
    "\n",
    "# Example scoring function\n",
    "def calculate_score(data_item):\n",
    "    try:\n",
    "        messages = copy.deepcopy(messages_template)\n",
    "        messages[1][\"content\"] = messages[1][\"content\"].format_map({\"instruction\":data_item[\"instruction\"],\"response\":data_item[\"response\"]})\n",
    "        # score = request_gpt(messages)\n",
    "        score = request_doubao(messages)\n",
    "    except Exception as e:\n",
    "        print(\"calculate\", e)\n",
    "        score = 3\n",
    "    return score\n",
    "\n",
    "def process_item(data_item):\n",
    "    return calculate_score(data_item)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes to use\n",
    "    num_processes = mp.cpu_count()\n",
    "    root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "    datas = []\n",
    "    settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"]\n",
    "    for setting in settings:\n",
    "        datas = []\n",
    "        for i in range(10):\n",
    "            datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "        datas = concatenate_datasets(datas)\n",
    "        datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "        # Use the pool to map the function to the dataset\n",
    "        with mp.Pool(processes=5) as pool:\n",
    "            scores = pool.map(process_item, tqdm(datas))\n",
    "        \n",
    "        # scores=[]\n",
    "        # for data in tqdm(datas):\n",
    "        #     scores.append(process_item(data))\n",
    "\n",
    "        # Calculate the average score\n",
    "        average_score = sum(scores) / len(scores)\n",
    "        print(f'Average Quality Score: {average_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对于 public 数据集的覆盖度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:22<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_med The volume of the UMAP-reduced embeddings is: 0.7521343888558817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:25<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_med The volume of the UMAP-reduced embeddings is: 0.8904378765485443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med The volume of the UMAP-reduced embeddings is: 0.7109492000108024\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "# settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"] [0.75,0.88,0.72,0.8095]\n",
    "# settings = [\"iid2niid_code_public_filter\"] # 0.8095\n",
    "settings = [\"pos_med\", \"random_med\", \"iid2niid_med\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    volume = coverage(datas_embeddings, data_concated_embeddings)\n",
    "    # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    print(f\"{setting} The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对于code数据集的覆盖度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:21<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_med The volume of the UMAP-reduced embeddings is: 0.9348416885086582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:26<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_med The volume of the UMAP-reduced embeddings is: 0.79404436071681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med The volume of the UMAP-reduced embeddings is: 0.883980716773177\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "# settings = [\"pos\", \"random\", \"iid2niid_code_public\"] [0.9396, 0.8512, 0.9531, 0.9766]\n",
    "# settings = [\"iid2niid_code_public_filter\"] # 0.9766\n",
    "# settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\"]\n",
    "settings = [\"pos_med\", \"random_med\", \"iid2niid_med\"]\n",
    "# settings = [\"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "# code_embeddings = data_concated_embeddings[:20022]\n",
    "med_embeddings = data_concated_embeddings[96794:96794+33956]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    volume = coverage(datas_embeddings, med_embeddings)\n",
    "    # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    print(f\"{setting} The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 10/10 [00:03<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_0.01 The volume of the UMAP-reduced embeddings is: 0.6849981924442328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 10/10 [00:03<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_0.1 The volume of the UMAP-reduced embeddings is: 0.6849981924442328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_1 The volume of the UMAP-reduced embeddings is: 0.6849981924442328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_10 The volume of the UMAP-reduced embeddings is: 0.6849981924442328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_med_0.01 The volume of the UMAP-reduced embeddings is: 0.6136875088996804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_med_0.1 The volume of the UMAP-reduced embeddings is: 0.6136875088996804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_med_1 The volume of the UMAP-reduced embeddings is: 0.6136875088996804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niid_med_10 The volume of the UMAP-reduced embeddings is: 0.6136875088996804\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\", \"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    volume = coverage(datas_embeddings, data_concated_embeddings)\n",
    "    # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    print(f\"{setting} The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维到同一平面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.99it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings for alpha=0.01 is: 420.6180865435744\n",
      "The volume of the UMAP-reduced embeddings for alpha=0.1 is: 420.784624893828\n",
      "The volume of the UMAP-reduced embeddings for alpha=1 is: 420.7824662411095\n",
      "The volume of the UMAP-reduced embeddings for alpha=10 is: 420.4096929098317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"64\"\n",
    "\n",
    "def umap_embeddings_volume(embeddings, umap_params=None):\n",
    "    if umap_params is None:\n",
    "        umap_params = {'n_components': 2, 'metric': 'cosine'}\n",
    "    \n",
    "    # 初始化 UMAP 降维模型\n",
    "    reducer = umap.UMAP(**umap_params)\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # 将降维后的数据按原分割点分开\n",
    "    split_indices = np.cumsum([len(e) for e in all_datas_embeddings])\n",
    "    embeddings_split = np.split(low_dim_embeddings, split_indices[:-1])\n",
    "\n",
    "    volumes = []\n",
    "    for embeddings in embeddings_split:\n",
    "        # 计算凸包\n",
    "        hull = ConvexHull(embeddings)\n",
    "        volumes.append(hull.volume)\n",
    "    \n",
    "    for alpha, volume in zip(alpha_values, volumes):\n",
    "        print(f\"The volume of the UMAP-reduced embeddings for alpha={alpha} is: {volume}\")\n",
    "\n",
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "alpha_values = [0.01, 0.1, 1, 10]\n",
    "all_datas_embeddings = []\n",
    "\n",
    "import random\n",
    "for alpha in alpha_values:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/niid_{alpha}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 5000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    all_datas_embeddings.append(datas_embeddings)\n",
    "\n",
    "# 将所有嵌入向量合并\n",
    "all_embeddings = np.vstack(all_datas_embeddings)\n",
    "\n",
    "umap_embeddings_volume(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.67it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.52it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.83it/s]\n",
      "/home/tiger/.local/lib/python3.9/site-packages/cuml/internals/api_decorators.py:344: UserWarning: Starting from version 22.04, the default method of TSNE is 'fft'.\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'handle': <pylibraft.common.handle.Handle object at 0x7fe060f2e180>, 'verbose': 4, 'output_type': 'input', 'output_mem_type': <MemoryType.device: 1>, '_input_type': None, '_input_mem_type': None, 'target_dtype': None, 'n_features_in_': None, 'n_components': 2, 'perplexity': 50, 'early_exaggeration': 12.0, 'late_exaggeration': 1.0, 'learning_rate': 200.0, 'n_iter': 1000, 'n_iter_without_progress': 300, 'min_grad_norm': 1e-07, 'metric': 'cosine', 'metric_params': None, 'init': 'random', 'random_state': None, 'method': 'fft', 'angle': 0.5, 'n_neighbors': 300, 'perplexity_max_iter': 100, 'exaggeration_iter': 250, 'pre_momentum': 0.5, 'post_momentum': 0.8, 'learning_rate_method': 'adaptive', 'epssq': 0.0025, 'perplexity_tol': 1e-05, 'min_gain': 0.01, 'pre_learning_rate': 200.0, 'post_learning_rate': 400.0, 'square_distances': True, 'X_m': CumlArrayDescriptorMeta(input_type=None, values={None: None}), 'embedding_': CumlArrayDescriptorMeta(input_type=None, values={None: None}), 'sparse_fit': False, 'precomputed_knn': None}\n",
      "[W] [17:07:48.837461] # of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "The volume of the t-SNE-reduced embeddings for alpha=0.01 is: 9609.673245956921\n",
      "The volume of the t-SNE-reduced embeddings for alpha=0.1 is: 9509.641987760424\n",
      "The volume of the t-SNE-reduced embeddings for alpha=1 is: 9397.102232173509\n",
      "The volume of the t-SNE-reduced embeddings for alpha=10 is: 9487.350566934909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "from cuml.manifold import TSNE\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"64\"\n",
    "\n",
    "def tsne_embeddings_volume(embeddings, split_indices):\n",
    "    # 初始化 t-SNE 降维模型\n",
    "    resolver = TSNE(n_components=2, metric='cosine', perplexity=50, n_neighbors=300)\n",
    "    print(resolver.__dict__)\n",
    "    # 进行 t-SNE 降维\n",
    "    low_dim_embeddings = resolver.fit_transform(embeddings)\n",
    "\n",
    "    # 将降维后的数据按原分割点分开\n",
    "    embeddings_split = np.split(low_dim_embeddings, split_indices[:-1])\n",
    "\n",
    "    volumes = []\n",
    "    for embeddings in embeddings_split:\n",
    "        # 计算凸包\n",
    "        hull = ConvexHull(embeddings)\n",
    "        volumes.append(hull.volume)\n",
    "    \n",
    "    return volumes\n",
    "\n",
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "alpha_values = [0.01, 0.1, 1, 10]\n",
    "all_datas_embeddings = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/niid_{alpha}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 5000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    all_datas_embeddings.append(datas_embeddings)\n",
    "\n",
    "# 将所有嵌入向量合并\n",
    "all_embeddings = np.vstack(all_datas_embeddings)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "all_embeddings = scaler.fit_transform(all_embeddings)\n",
    "\n",
    "# 计算分割索引\n",
    "split_indices = np.cumsum([len(e) for e in all_datas_embeddings])\n",
    "\n",
    "volumes = tsne_embeddings_volume(all_embeddings, split_indices)\n",
    "\n",
    "# 打印不同 niid程度数据集的凸包体积\n",
    "for alpha, volume in zip(alpha_values, volumes):\n",
    "    print(f\"The volume of the t-SNE-reduced embeddings for alpha={alpha} is: {volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iid 的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:13<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_anchor_nodup_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 1171.5674569446574\n"
     ]
    }
   ],
   "source": [
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prototype based niid 的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 604.2912879650961\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_prototype_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [01:30<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 1882.2693424905829\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_anchor_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
