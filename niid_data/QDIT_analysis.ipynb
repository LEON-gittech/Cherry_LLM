{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造niid和iid的jsonl，看看其对应的广度和质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]\n",
    "\n",
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example\n",
    "\n",
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "        # if \"fin\" in dataset_name: dataset = dataset.shuffle(seed=42).select(range(51000))\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        # dataset = dataset.shuffle(seed=42).select(range(51000))\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset\n",
    "\n",
    "processed_data = []\n",
    "for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\",\"tatsu-lab/alpaca\",\"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,general_data,math_data]):\n",
    "# for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\", \"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,math_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)\n",
    "    \n",
    "data_concated = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 3*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"\",\n",
    "                  use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', model_kwargs={\"torch_dtype\":torch.bfloat16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_block(A,B,block_size=50000):\n",
    "    C = torch.zeros(A.size(0), B.size(1))\n",
    "    # 进行分块矩阵乘法\n",
    "    for i in range(0, A.size(0), block_size):\n",
    "        for j in range(0, B.size(1), block_size):\n",
    "            for k in range(0, A.size(1), block_size):\n",
    "                # 计算分块索引\n",
    "                i_end = min(i + block_size, A.size(0))\n",
    "                j_end = min(j + block_size, B.size(1))\n",
    "                k_end = min(k + block_size, A.size(1))\n",
    "                # 执行子块乘法并累加到结果矩阵中\n",
    "                C[i:i_end, j:j_end] += torch.mm(A[i:i_end, k:k_end].cuda(), B[k:k_end, j:j_end].cuda()).cpu()\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity as cosine_similarity\n",
    "\n",
    "def coverage(A, V):\n",
    "    # 将输入转换为张量\n",
    "    A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "    V_tensor = torch.tensor(V, dtype=torch.float32)\n",
    "    # 计算集合A的覆盖广度\n",
    "    # similarities = matmul_block(V_tensor,A_tensor.T)\n",
    "    similarities = torch.matmul(V_tensor, A_tensor.T)\n",
    "    # 计算每个v的最大相似度\n",
    "    max_similarities = torch.max(similarities, dim=1).values\n",
    "    # 计算总相似度\n",
    "    total_similarity = torch.sum(max_similarities).item()/len(max_similarities)\n",
    "    return total_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings:   0%|          | 0/531 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc61-p1a-t455-n036:904320:904320 [0] NCCL INFO cudaDriverVersion 12020\n",
      "dc61-p1a-t455-n036:904320:904320 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "dc61-p1a-t455-n036:904320:904320 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "dc61-p1a-t455-n036:904320:904320 [0] NCCL INFO Bootstrap : Using eth0:fdbd:dc61:1a:455::36<0>\n",
      "dc61-p1a-t455-n036:904320:904320 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "NCCL version 2.20.5+cuda12.4\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NCCL_IB_HCA set to mlx5\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/RoCE [1]mlx5_1:1/RoCE [2]mlx5_2:1/RoCE [3]mlx5_3:1/RoCE [RO]; OOB eth0:fdbd:dc61:1a:455::36<0>\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Using non-device net plugin version 0\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Using network IB\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Using non-device net plugin version 0\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Using network IB\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Using non-device net plugin version 0\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Using network IB\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO comm 0xd187200 rank 2 nranks 3 cudaDev 2 nvmlDev 2 busId cc000 commId 0xb5931cb246dfb09b - Init START\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO comm 0x10eed6f0 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 96000 commId 0xb5931cb246dfb09b - Init START\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO comm 0xd183160 rank 1 nranks 3 cudaDev 1 nvmlDev 1 busId c8000 commId 0xb5931cb246dfb09b - Init START\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff,00000000,ffffffff,00000000\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff,00000000,ffffffff,00000000\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff,00000000\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO comm 0x10eed6f0 rank 0 nRanks 3 nNodes 1 localRanks 3 localRank 0 MNNVL 0\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO comm 0xd187200 rank 2 nRanks 3 nNodes 1 localRanks 3 localRank 2 MNNVL 0\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 00/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO comm 0xd183160 rank 1 nRanks 3 nNodes 1 localRanks 3 localRank 1 MNNVL 0\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 01/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 02/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 03/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 04/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 05/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 06/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 07/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 08/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 09/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] -1/-1/-1->2->1 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1 [4] -1/-1/-1->2->1 [5] -1/-1/-1->2->1 [6] -1/-1/-1->2->1 [7] -1/-1/-1->2->1 [8] -1/-1/-1->2->1 [9] -1/-1/-1->2->1 [10] -1/-1/-1->2->1 [11] -1/-1/-1->2->1 [12] -1/-1/-1->2->1 [13] -1/-1/-1->2->1 [14] -1/-1/-1->2->1 [15] -1/-1/-1->2->1 [16] -1/-1/-1->2->1 [17] -1/-1/-1->2->1 [18] -1/-1/-1->2->1 [19] -1/-1/-1->2->1 [20] -1/-1/-1->2->1 [21] -1/-1/-1->2->1 [22] -1/-1/-1->2->1 [23] -1/-1/-1->2->1\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 10/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 11/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO P2P Chunksize set to 524288\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 12/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 13/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 14/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 15/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 16/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 17/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 18/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 19/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 20/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 21/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 22/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 23/24 :    0   1   2\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 00/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 08/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 09/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 11/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 12/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 14/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 15/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 20/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 21/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 23/0 : 2[2] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Connected all rings\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Connected all rings\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Connected all rings\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/direct pointer/read\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO Connected all trees\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO Connected all trees\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO 24 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO 24 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO Connected all trees\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO 24 coll channels, 0 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\n",
      "dc61-p1a-t455-n036:904320:1095494 [2] NCCL INFO comm 0xd187200 rank 2 nranks 3 cudaDev 2 nvmlDev 2 busId cc000 commId 0xb5931cb246dfb09b - Init COMPLETE\n",
      "dc61-p1a-t455-n036:904320:1095493 [1] NCCL INFO comm 0xd183160 rank 1 nranks 3 cudaDev 1 nvmlDev 1 busId c8000 commId 0xb5931cb246dfb09b - Init COMPLETE\n",
      "dc61-p1a-t455-n036:904320:1095492 [0] NCCL INFO comm 0x10eed6f0 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 96000 commId 0xb5931cb246dfb09b - Init COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "Inference Embeddings: 100%|██████████| 531/531 [04:36<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "data_concated_embeddings = model.encode(data_concated[\"instruction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## niid的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from scipy.spatial import ConvexHull\n",
    "from cuml.manifold import UMAP, TSNE\n",
    "\n",
    "def umap_embeddings_volume(embeddings):\n",
    "    # 初始化 UMAP 降维模型\n",
    "    reducer = umap.UMAP(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def cuml_tsne_embeddings_volume(embeddings):\n",
    "    reducer = TSNE(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def cuml_umap_embeddings_volume(embeddings):\n",
    "    reducer = UMAP(n_components=2, metric='cosine')\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "    # 将降维后的数据转换为 NumPy 数组\n",
    "    points = np.array(low_dim_embeddings)\n",
    "    # 计算凸包\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算数据平均质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.265625\n"
     ]
    }
   ],
   "source": [
    "reward_name = \"/mnt/bn/data-tns-live-llm/leon/datasets/reward-model-deberta-v3-large-v2/\"\n",
    "device = \"cuda\"\n",
    "rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name, torch_dtype=torch.bfloat16).to(device), AutoTokenizer.from_pretrained(reward_name)\n",
    "# rank_model, tokenizer = AutoModelForSequenceClassification.from_pretrained(reward_name, load_in_4bit=True), AutoTokenizer.from_pretrained(reward_name)\n",
    "question, answer = \"Explain nuclear fusion like I am five\", \"Nuclear fusion is the process by which two or more protons and neutrons combine to form a single nucleus. It is a very important process in the universe, as it is the source of energy for stars and galaxies. Nuclear fusion is also a key process in the production of energy for nuclear power plants.\"\n",
    "inputs = tokenizer(question, answer, return_tensors='pt').to(device)\n",
    "score = rank_model(**inputs).logits[0].detach()\n",
    "print(float(score))\n",
    "\n",
    "def quality_evaluation(datas):\n",
    "    score = 0\n",
    "    cnt = 0\n",
    "    result_list = []\n",
    "    for element in tqdm(datas):\n",
    "        instruction = element['instruction']\n",
    "        _input = ''\n",
    "        if 'input' in element.keys():\n",
    "            _input = element['input']\n",
    "        _output = element['response']\n",
    "        question = ''\n",
    "        if _input == '':\n",
    "            question = instruction\n",
    "        else:\n",
    "            question = instruction + '\\n' +_input\n",
    "        \n",
    "        answer = _output\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(question, answer, return_tensors='pt').to(device)\n",
    "            score += rank_model(**inputs).logits[0].detach()\n",
    "            cnt +=1\n",
    "        except:\n",
    "            print(instruction)\n",
    "            print(_output)\n",
    "            continue\n",
    "        \n",
    "    print(score/cnt)\n",
    "        # final_result = {'instruction':instruction,'input':_input,'response':_output,'reward_score':float(score)}\n",
    "        # result_list.append(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "    quality_evaluation(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\", \"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "    quality_evaluation(datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用gpt评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Instruction: write a joke about biden Input: None Response: oh biden, sleepy joe!'}, {'role': 'user', 'content': 'Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "messages_template =[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Instruction: {instruction} Input: None Response: {response}\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"\"\"\n",
    "    },\n",
    "    ]\n",
    "\n",
    "messages_template[0][\"content\"] = messages_template[0][\"content\"].format_map({\"instruction\":\"write a joke about biden\",\"response\":\"oh biden, sleepy joe!\"})\n",
    "print(messages_template)\n",
    "MODEL = \"gpt-4o-2024-05-13\"\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "data = {\n",
    "    \"model\": MODEL,\n",
    "    \"messages\": messages_template,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 256,\n",
    "}\n",
    "headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "# data = {k: v for k, v in data.items() if v is not None}\n",
    "# data = json.dumps(data)\n",
    "# url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "# response = requests.post(url, data=data, headers=headers)\n",
    "# print(response.content)\n",
    "# score = int(response.json()[\"choices\"][0][\"message\"][\"content\"][0])\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- standard request -----\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "client = Ark(base_url=\"https://ark.cn-beijing.volces.com/api/v3\",api_key=\"\")\n",
    "\n",
    "# Non-streaming:\n",
    "print(\"----- standard request -----\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Instruction: Remove all the punctuation from a given string \"Welcome to the world of computers!\", Response: 'import string\\n\\ndef remove_punctuation(text):\\n    punctuations = string.punctuation\\n    no_punct = \"\"\\n    for char in text:\\n        if char not in punctuations:\\n            no_punct += char\\n    return no_punct\\n\\nif __name__ == \\'__main__\\':\\n    text = \\'Welcome to the world of computers!\\'\\n    print(remove_punctuation(text))'\"\"\"},\n",
    "    ],\n",
    "    # temperature=0.1,\n",
    "    # max_tokens=256\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def request_doubao(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ep-20240804150426-2vkvx\",\n",
    "        messages = messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    try:\n",
    "        response = response.choices[0].message.content\n",
    "        match = re.search(r'\\d+', response)\n",
    "        score = int(match.group(0))\n",
    "    except Exception as e: \n",
    "        print(\"request\",response)\n",
    "        score = 3\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "\n",
    "messages_template =[\n",
    "{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following. Please rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the accuracy. Please first output a single line containing the value indicating the scores. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.\"\"\"\n",
    "},\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"Instruciton: {instruction} Response: {response}\"\"\"\n",
    "},\n",
    "]\n",
    "\n",
    "def request_gpt(messages):\n",
    "    data = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 256,\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json', 'Caller': 'leon.kepler'}\n",
    "    data = {k: v for k, v in data.items() if v is not None}\n",
    "    data = json.dumps(data)\n",
    "    url = f\"https://swzkkd0h.us-east-fn.bytedance.net/gpt/openapi/online/v2/crawl\"\n",
    "    # response = requests.post(url, data=data, headers=headers)\n",
    "    # if \"choices\" not in response.json().keys(): response = requests.post(url, data=data, headers=headers)\n",
    "    try: \n",
    "        response = requests.post(url, data=data, headers=headers)\n",
    "        response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        match = re.search(r'\\d+', response)\n",
    "        score = int(match.group(0))\n",
    "        # score = int(response.json()[\"choices\"][0][\"message\"][\"content\"][0])\n",
    "    except Exception as e: \n",
    "        print(\"request\",response)\n",
    "        score = 3\n",
    "    return score\n",
    "\n",
    "# Example scoring function\n",
    "def calculate_score(data_item):\n",
    "    try:\n",
    "        messages = copy.deepcopy(messages_template)\n",
    "        messages[1][\"content\"] = messages[1][\"content\"].format_map({\"instruction\":data_item[\"instruction\"],\"response\":data_item[\"response\"]})\n",
    "        # score = request_gpt(messages)\n",
    "        score = request_doubao(messages)\n",
    "    except Exception as e:\n",
    "        print(\"calculate\", e)\n",
    "        score = 3\n",
    "    return score\n",
    "\n",
    "def process_item(data_item):\n",
    "    return calculate_score(data_item)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes to use\n",
    "    num_processes = mp.cpu_count()\n",
    "    root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "    datas = []\n",
    "    settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"]\n",
    "    for setting in settings:\n",
    "        datas = []\n",
    "        for i in range(10):\n",
    "            datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "        datas = concatenate_datasets(datas)\n",
    "        datas = datas.select(random.sample(range(len(datas)), 1000))\n",
    "        # Use the pool to map the function to the dataset\n",
    "        with mp.Pool(processes=5) as pool:\n",
    "            scores = pool.map(process_item, tqdm(datas))\n",
    "        \n",
    "        # scores=[]\n",
    "        # for data in tqdm(datas):\n",
    "        #     scores.append(process_item(data))\n",
    "\n",
    "        # Calculate the average score\n",
    "        average_score = sum(scores) / len(scores)\n",
    "        print(f'Average Quality Score: {average_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对于 public 数据集的覆盖度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin The volume of the UMAP-reduced embeddings is: 0.654271125386062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:13<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin_filter The volume of the UMAP-reduced embeddings is: 0.7089580499756947\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "# settings = [\"pos\", \"random\", \"iid2niid_code\", \"iid2niid_code_public\"] [0.75,0.88,0.72,0.8095]\n",
    "# settings = [\"iid2niid_code_public_filter\"] # 0.8095\n",
    "# settings = [\"pos_med\", \"random_med\", \"iid2niid_med\"]\n",
    "# settings = [\"public_10\", \"public_1\", \"public_0.1\",\"public_0.01\"]\n",
    "# settings = [\"niid_pos_public\", \"iid_pos_public\"]\n",
    "# settings = [\"iid2niid_math\",\"iid2niid_math_filter\"]\n",
    "settings = [\"iid2niid_fin\",\"iid2niid_fin_filter\"]\n",
    "# settings = [\"pos_math\"]\n",
    "# settings = [\"fed_code_only\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    volume = coverage(datas_embeddings, data_concated_embeddings)\n",
    "    # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    print(f\"{setting} The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对于各 domain 数据集的覆盖度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 54/54 [00:22<00:00,  2.41it/s]\n",
      "  6%|▋         | 1/16 [00:23<05:55, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_code_4000's coverage is: 0.9767145788070623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 41/41 [00:15<00:00,  2.73it/s]\n",
      " 12%|█▎        | 2/16 [00:45<05:12, 22.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_code_3000's coverage is: 0.9647033770664769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 28/28 [00:09<00:00,  2.95it/s]\n",
      " 19%|█▉        | 3/16 [00:55<03:38, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_code_2000's coverage is: 0.9443467654330236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:04<00:00,  3.46it/s]\n",
      " 25%|██▌       | 4/16 [01:00<02:25, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_code_1000's coverage is: 0.9083204763822296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 54/54 [00:26<00:00,  2.07it/s]\n",
      " 31%|███▏      | 5/16 [01:33<03:38, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med_4000's coverage is: 0.8828922816043293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 41/41 [00:17<00:00,  2.30it/s]\n",
      " 38%|███▊      | 6/16 [01:54<03:22, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med_3000's coverage is: 0.859376495545575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 28/28 [00:11<00:00,  2.40it/s]\n",
      " 44%|████▍     | 7/16 [02:15<03:04, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med_2000's coverage is: 0.8287683146811957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:05<00:00,  2.61it/s]\n",
      " 50%|█████     | 8/16 [02:22<02:08, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_med_1000's coverage is: 0.7876623242158739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 54/54 [00:16<00:00,  3.22it/s]\n",
      " 56%|█████▋    | 9/16 [02:43<02:02, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin_4000's coverage is: 0.9736504682696816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 41/41 [00:12<00:00,  3.37it/s]\n",
      " 62%|██████▎   | 10/16 [02:58<01:41, 16.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin_3000's coverage is: 0.9667212655655708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 28/28 [00:08<00:00,  3.43it/s]\n",
      " 69%|██████▉   | 11/16 [03:09<01:14, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin_2000's coverage is: 0.956135537696035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:04<00:00,  3.37it/s]\n",
      " 75%|███████▌  | 12/16 [03:14<00:48, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_fin_1000's coverage is: 0.9344662604855937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 54/54 [00:37<00:00,  1.43it/s]\n",
      " 81%|████████▏ | 13/16 [04:04<01:10, 23.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_math_4000's coverage is: 0.9152439750720275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 41/41 [00:28<00:00,  1.45it/s]\n",
      " 88%|████████▊ | 14/16 [04:50<01:00, 30.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_math_3000's coverage is: 0.9036248090770238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 28/28 [00:19<00:00,  1.41it/s]\n",
      " 94%|█████████▍| 15/16 [05:19<00:29, 29.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_math_2000's coverage is: 0.887758233066301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 15/15 [00:10<00:00,  1.45it/s]\n",
      "100%|██████████| 16/16 [05:33<00:00, 20.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid2niid_math_1000's coverage is: 0.862585553531908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "# settings = [\"pos\", \"random\", \"iid2niid_code_public\"] [0.9396, 0.8512, 0.9531, 0.9766]\n",
    "# settings = [\"iid2niid_code_public_filter\"] # 0.9766\n",
    "# settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\"]\n",
    "# settings = [\"pos_med\", \"random_med\", \"iid2niid_med\"]\n",
    "# settings = [\"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "# settings = [\"public_10\", \"public_1\", \"public_0.1\",\"public_0.01\"]\n",
    "# settings = [\"niid_pos_public\", \"iid_pos_public\"]\n",
    "# settings = [\"random_math\", \"iid2niid_math_filter\"]\n",
    "# settings = [\"random_fin\",\"pos_math\"]\n",
    "# settings = [\"iid2niid_math\",\"iid2niid_math_filter\"]\n",
    "# settings = [\"pos_nodup_math\"]\n",
    "# settings = [\"iid2niid_fin\",\"iid2niid_fin_filter\"]\n",
    "settings = [\n",
    "    \"iid2niid_code_4000\",\"iid2niid_code_3000\",\"iid2niid_code_2000\",\"iid2niid_code_1000\",\n",
    "    \"iid2niid_med_4000\",\"iid2niid_med_3000\",\"iid2niid_med_2000\",\"iid2niid_med_1000\",\n",
    "    \"iid2niid_fin_4000\",\"iid2niid_fin_3000\",\"iid2niid_fin_2000\",\"iid2niid_fin_1000\",\n",
    "    \"iid2niid_math_4000\",\"iid2niid_math_3000\",\"iid2niid_math_2000\",\"iid2niid_math_1000\",\n",
    "]\n",
    "\n",
    "# partial data\n",
    "# code_embeddings = data_concated_embeddings[:20022]\n",
    "# med_embeddings = data_concated_embeddings[71022:104977]\n",
    "# fin_embeddings = data_concated_embeddings[20022:71022]\n",
    "# math_embeddings = data_concated_embeddings[156979:207979]\n",
    "\n",
    "# full data\n",
    "code_embeddings = data_concated_embeddings[:20022]\n",
    "med_embeddings = data_concated_embeddings[96794:130749]\n",
    "fin_embeddings = data_concated_embeddings[20022:96794]\n",
    "math_embeddings = data_concated_embeddings[182751:]\n",
    "\n",
    "# names = [\"code\",\"med\",\"fin\",\"math\"]\n",
    "# domain_embeddings = [code_embeddings,med_embeddings,fin_embeddings,math_embeddings]\n",
    "\n",
    "# 下两行都要改！\n",
    "# names = [\"math\"]\n",
    "# domain_embedding_list = [math_embeddings] #这要根据 domain 进行修改\n",
    "\n",
    "for setting in tqdm(settings):\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "\n",
    "    if \"code\" in setting: domain_embeddings = code_embeddings\n",
    "    elif \"med\" in setting: domain_embeddings = med_embeddings\n",
    "    elif \"fin\" in setting: domain_embeddings = fin_embeddings\n",
    "    else: domain_embeddings = math_embeddings\n",
    "\n",
    "    volume = coverage(datas_embeddings, domain_embeddings)\n",
    "    print(f\"{setting}'s coverage is:\", volume)\n",
    "\n",
    "    # for name, domain_embeddings in zip(names, domain_embedding_list):\n",
    "    #     volume = coverage(datas_embeddings, domain_embeddings)\n",
    "    #     # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    #     print(f\"{setting}'s coverage is:\", volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "# settings = [\"niid_0.01\", \"niid_0.1\", \"niid_1\", \"niid_10\", \"niid_med_0.01\", \"niid_med_0.1\", \"niid_med_1\", \"niid_med_10\"]\n",
    "settings = [\"niid_pos_public\", \"iid_pos_public\"]\n",
    "for setting in settings:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/{setting}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    if len(datas) > 55000: datas = datas.select(random.sample(range(len(datas)), 55000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    volume = coverage(datas_embeddings, data_concated_embeddings)\n",
    "    # volume = umap_embeddings_volume(datas_embeddings)\n",
    "    print(f\"{setting} The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维到同一平面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.37it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.99it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings for alpha=0.01 is: 420.6180865435744\n",
      "The volume of the UMAP-reduced embeddings for alpha=0.1 is: 420.784624893828\n",
      "The volume of the UMAP-reduced embeddings for alpha=1 is: 420.7824662411095\n",
      "The volume of the UMAP-reduced embeddings for alpha=10 is: 420.4096929098317\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"64\"\n",
    "\n",
    "def umap_embeddings_volume(embeddings, umap_params=None):\n",
    "    if umap_params is None:\n",
    "        umap_params = {'n_components': 2, 'metric': 'cosine'}\n",
    "    \n",
    "    # 初始化 UMAP 降维模型\n",
    "    reducer = umap.UMAP(**umap_params)\n",
    "    # 进行 UMAP 降维\n",
    "    low_dim_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # 将降维后的数据按原分割点分开\n",
    "    split_indices = np.cumsum([len(e) for e in all_datas_embeddings])\n",
    "    embeddings_split = np.split(low_dim_embeddings, split_indices[:-1])\n",
    "\n",
    "    volumes = []\n",
    "    for embeddings in embeddings_split:\n",
    "        # 计算凸包\n",
    "        hull = ConvexHull(embeddings)\n",
    "        volumes.append(hull.volume)\n",
    "    \n",
    "    for alpha, volume in zip(alpha_values, volumes):\n",
    "        print(f\"The volume of the UMAP-reduced embeddings for alpha={alpha} is: {volume}\")\n",
    "\n",
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "alpha_values = [0.01, 0.1, 1, 10]\n",
    "all_datas_embeddings = []\n",
    "\n",
    "import random\n",
    "for alpha in alpha_values:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/niid_{alpha}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 5000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    all_datas_embeddings.append(datas_embeddings)\n",
    "\n",
    "# 将所有嵌入向量合并\n",
    "all_embeddings = np.vstack(all_datas_embeddings)\n",
    "\n",
    "umap_embeddings_volume(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.67it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.52it/s]\n",
      "Inference Embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.83it/s]\n",
      "/home/tiger/.local/lib/python3.9/site-packages/cuml/internals/api_decorators.py:344: UserWarning: Starting from version 22.04, the default method of TSNE is 'fft'.\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'handle': <pylibraft.common.handle.Handle object at 0x7fe060f2e180>, 'verbose': 4, 'output_type': 'input', 'output_mem_type': <MemoryType.device: 1>, '_input_type': None, '_input_mem_type': None, 'target_dtype': None, 'n_features_in_': None, 'n_components': 2, 'perplexity': 50, 'early_exaggeration': 12.0, 'late_exaggeration': 1.0, 'learning_rate': 200.0, 'n_iter': 1000, 'n_iter_without_progress': 300, 'min_grad_norm': 1e-07, 'metric': 'cosine', 'metric_params': None, 'init': 'random', 'random_state': None, 'method': 'fft', 'angle': 0.5, 'n_neighbors': 300, 'perplexity_max_iter': 100, 'exaggeration_iter': 250, 'pre_momentum': 0.5, 'post_momentum': 0.8, 'learning_rate_method': 'adaptive', 'epssq': 0.0025, 'perplexity_tol': 1e-05, 'min_gain': 0.01, 'pre_learning_rate': 200.0, 'post_learning_rate': 400.0, 'square_distances': True, 'X_m': CumlArrayDescriptorMeta(input_type=None, values={None: None}), 'embedding_': CumlArrayDescriptorMeta(input_type=None, values={None: None}), 'sparse_fit': False, 'precomputed_knn': None}\n",
      "[W] [17:07:48.837461] # of Nearest Neighbors should be at least 3 * perplexity. Your results might be a bit strange...\n",
      "The volume of the t-SNE-reduced embeddings for alpha=0.01 is: 9609.673245956921\n",
      "The volume of the t-SNE-reduced embeddings for alpha=0.1 is: 9509.641987760424\n",
      "The volume of the t-SNE-reduced embeddings for alpha=1 is: 9397.102232173509\n",
      "The volume of the t-SNE-reduced embeddings for alpha=10 is: 9487.350566934909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import random\n",
    "from cuml.manifold import TSNE\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"64\"\n",
    "\n",
    "def tsne_embeddings_volume(embeddings, split_indices):\n",
    "    # 初始化 t-SNE 降维模型\n",
    "    resolver = TSNE(n_components=2, metric='cosine', perplexity=50, n_neighbors=300)\n",
    "    print(resolver.__dict__)\n",
    "    # 进行 t-SNE 降维\n",
    "    low_dim_embeddings = resolver.fit_transform(embeddings)\n",
    "\n",
    "    # 将降维后的数据按原分割点分开\n",
    "    embeddings_split = np.split(low_dim_embeddings, split_indices[:-1])\n",
    "\n",
    "    volumes = []\n",
    "    for embeddings in embeddings_split:\n",
    "        # 计算凸包\n",
    "        hull = ConvexHull(embeddings)\n",
    "        volumes.append(hull.volume)\n",
    "    \n",
    "    return volumes\n",
    "\n",
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "alpha_values = [0.01, 0.1, 1, 10]\n",
    "all_datas_embeddings = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    datas = []\n",
    "    for i in range(10):\n",
    "        datas.append(load_from_disk(f\"{root}/niid_{alpha}_{i}.parquet\"))\n",
    "    datas = concatenate_datasets(datas)\n",
    "    datas = datas.select(random.sample(range(len(datas)), 5000))\n",
    "    datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "    all_datas_embeddings.append(datas_embeddings)\n",
    "\n",
    "# 将所有嵌入向量合并\n",
    "all_embeddings = np.vstack(all_datas_embeddings)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "all_embeddings = scaler.fit_transform(all_embeddings)\n",
    "\n",
    "# 计算分割索引\n",
    "split_indices = np.cumsum([len(e) for e in all_datas_embeddings])\n",
    "\n",
    "volumes = tsne_embeddings_volume(all_embeddings, split_indices)\n",
    "\n",
    "# 打印不同 niid程度数据集的凸包体积\n",
    "for alpha, volume in zip(alpha_values, volumes):\n",
    "    print(f\"The volume of the t-SNE-reduced embeddings for alpha={alpha} is: {volume}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iid 的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:13<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_anchor_nodup_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 1171.5674569446574\n"
     ]
    }
   ],
   "source": [
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prototype based niid 的广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 604.2912879650961\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_prototype_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 25/25 [01:30<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the UMAP-reduced embeddings is: 1882.2693424905829\n"
     ]
    }
   ],
   "source": [
    "root = \"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/\"\n",
    "datas = []\n",
    "for i in range(10):\n",
    "    datas.append(load_from_disk(f\"{root}/niid_anchor_public_{i}.parquet\"))\n",
    "datas = concatenate_datasets(datas)\n",
    "print(len(datas))\n",
    "datas_embeddings = model.encode(datas[\"instruction\"])\n",
    "# 调用函数计算 UMAP 降维后凸包的体积\n",
    "volume = umap_embeddings_volume(datas_embeddings)\n",
    "print(\"The volume of the UMAP-reduced embeddings is:\", volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
