{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]\n",
    "\n",
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example\n",
    "\n",
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        # dataset = dataset.shuffle(seed=42).select(range(51000))\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "# 这块一定要注意!!! name 和datasest的顺序都要改\n",
    "for name, dataset in zip([\"medalpaca/medical_meadow_medical_flashcards\",\"lucasmccabe-lmi/CodeAlpaca-20k\",\"TIGER-Lab/MathInstruct\",\"FinGPT/fingpt-sentiment-train\",\"tatsu-lab/alpaca\",],[med_data,code_data,math_data,fin_data,general_data]):\n",
    "# for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\",\"FinGPT/fingpt-sentiment-train\",\"medalpaca/medical_meadow_medical_flashcards\", \"TIGER-Lab/MathInstruct\"],[code_data,fin_data,med_data,math_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    # if \"fin\" in name: \n",
    "    #     tmp = tmp.shuffle(seed=42).select(range(51000))\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33955 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33955 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "属于\n",
      "\n",
      "理由：这个问题涉及到上肢（arm/elbow）的损伤类型，特别是导致肱骨髁上骨折（supracondylar fractures）的常见原因。这属于解剖学（anatomy）和临床知识（clinical knowledge）的范畴，因为它涉及到特定部位的解剖结构和常见的临床损伤模式。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "instructions = processed_data[0][\"instruction\"]\n",
    "responses = processed_data[0][\"response\"]\n",
    "\n",
    "# 定义判断主题的提示信息\n",
    "def check_topic(instruction):\n",
    "    prompt = f\"\"\"\n",
    "    你是一名医学专家。请判断以下医学指令是否属于以下主题之一，并给出理由：\n",
    "    - clinical knowledge\n",
    "    - anatomy\n",
    "    - medical genetics\n",
    "\n",
    "    ## Requirement: 如果属于以上任何一个主题，请回答“属于”；否则，请回答“不属于”.\n",
    "\n",
    "    ## Input: \"{instruction}\"\n",
    "    ## Output:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"gpt-4o-2024-08-06\",  # 使用GPT-4模型\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "        temperature=0.5,  # 确保答案稳定\n",
    "        n=1,\n",
    "        # stop=[\"\\n\"],\n",
    "    )\n",
    "    # 提取回答结果\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    print(answer)\n",
    "    return answer == \"属于\"\n",
    "\n",
    "# 分配两个列表：包含指定主题的和不包含指定主题的\n",
    "included_topics = []\n",
    "excluded_topics = []\n",
    "\n",
    "# 对每条数据进行主题判断并分类\n",
    "for instruction, response in tqdm(zip(instructions, responses), total=len(instructions)):\n",
    "    # print(instruction)\n",
    "    if check_topic(instruction):\n",
    "        included_topics.append({\"instruction\": instruction, \"response\": response})\n",
    "    else:\n",
    "        excluded_topics.append({\"instruction\": instruction, \"response\": response})\n",
    "    break\n",
    "\n",
    "# 输出结果\n",
    "# print(\"包含指定主题的指令集：\", len(included_topics))\n",
    "# print(\"不包含指定主题的指令集：\", len(excluded_topics))\n",
    "\n",
    "# # 返回结果以便于进一步使用\n",
    "# print(included_topics[0])\n",
    "# print(excluded_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = processed_data[0][\"instruction\"]\n",
    "responses = processed_data[0][\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a medical professional. Please determine whether the following medical instructions fall under one of the following topics:\n",
      "- clinical knowledge\n",
      "- anatomy\n",
      "- medical genetics<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "\n",
      "Inpute: \"What type of injury to the arm/elbow most often leads to supracondylar fractures?\"\n",
      "If you belong to any of the above topics, please answer \"Yes\"; otherwise, please answer \"No\".\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a medical professional. Please determine whether the following medical instructions fall under one of the following topics:\n",
    "- clinical knowledge\n",
    "- anatomy\n",
    "- medical genetics\"\"\"\n",
    "\n",
    "instruction = f\"{instructions[0]}\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Inpute: \"{instruction}\"\n",
    "If you belong to any of the above topics, please answer \"Yes\"; otherwise, please answer \"No\".\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_prompt}<|eot_id|>\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt.format_map({\"instruction\": ins}) for ins in instructions[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 11:09:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/mnt/bn/merlin-datavolume-tsy/leon/checkpoints/Llama3.1-8B-Chinese-Chat', speculative_config=None, tokenizer='/mnt/bn/merlin-datavolume-tsy/leon/checkpoints/Llama3.1-8B-Chinese-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/bn/merlin-datavolume-tsy/leon/checkpoints/Llama3.1-8B-Chinese-Chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 11-14 11:09:13 model_runner.py:1056] Starting to load model /mnt/bn/merlin-datavolume-tsy/leon/checkpoints/Llama3.1-8B-Chinese-Chat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4a83f7c47b4ff8a892aa693d0c991c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 11:09:16 model_runner.py:1067] Loading model weights took 14.9888 GB\n",
      "INFO 11-14 11:09:17 gpu_executor.py:122] # GPU blocks: 20011, # CPU blocks: 2048\n",
      "INFO 11-14 11:09:17 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 156.34x\n",
      "INFO 11-14 11:09:18 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-14 11:09:18 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-14 11:09:34 model_runner.py:1523] Graph capturing finished in 16 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/mnt/bn/merlin-datavolume-tsy/leon/checkpoints/Llama3.1-8B-Chinese-Chat\", tensor_parallel_size=1, \n",
    "            dtype=torch.bfloat16, trust_remote_code=True, enable_lora=False, max_model_len=2048, gpu_memory_utilization=0.8)\n",
    "\n",
    "sampling_params = SamplingParams(top_p=1, max_tokens=10, temperature=0) # memory extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s, est. speed input: 21.93 toks/s, output: 73.08 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(\"介绍一下 python\", sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 的内置函数 `enumerate` 的用法\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Yes, this question falls under\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "583f1f30-a6f7-4920-aace-e5e8f151fc64",
  "filePath": "/mnt/bn/data-tns-live-llm/leon/Cherry_LLM/niid_data/divide_task.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
