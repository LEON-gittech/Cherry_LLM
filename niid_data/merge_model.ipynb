{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-23 13:12:21,302] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /home/tiger/.triton/autotune: No such file or directory\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1281.35 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 65.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1280.99 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 65.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_code_public_filter_20000_fedavgm_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_fedavgm_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_code_public_filter_20000_fedprox_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/code_fedprox_merged\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1437.26 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 55.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1437.16 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 62.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1436.96 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 39.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.347 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1441.36 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_4000_20000_fedavg_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_4000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_3000_20000_fedavg_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_3000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_2000_20000_fedavg_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_2000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_1000_20000_fedavg_c10s2_i10_b16a2_l2048_r32a64_f0\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/iid2niid_math_1000_merged\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.109 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 9.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1334.15 out of 1842.29 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 74.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "[2024-08-15 20:12:40,483] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/random_client_0/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/random_client_0_merged\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "data = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/random.parquet\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# niid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_10_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_10\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_1_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_1\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_0.1_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_0.1\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_0.01_20000_fedavg_c10s2_i20_b16a1_l2048_r128a256_f0/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed/niid_math_0.01\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-80GB. Max memory: 79.151 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1418.08 out of 2015.23 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 53.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_med/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_med_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_1000/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_1000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_4000/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_4000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_2000/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_2000_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_5000/\", dtype = torch.bfloat16, load_in_4bit=True)\n",
    "model.save_pretrained_merged(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_5000_merged\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
