{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example\n",
    "    \n",
    "def labeling(example, label):\n",
    "    example[\"label\"] = label\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        # dataset = dataset.shuffle(seed=42).select(range(51000))\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "# 这里前后都要改\n",
    "for name, dataset in zip([\"lucasmccabe-lmi/CodeAlpaca-20k\", \"TIGER-Lab/MathInstruct\",\"FinGPT/fingpt-sentiment-train\", \"medalpaca/medical_meadow_medical_flashcards\",\"tatsu-lab/alpaca\"],[code_data, math_data,fin_data,med_data,general_data]): # 这里前后都要改\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    # if \"fin\" in name: \n",
    "    #     tmp = tmp.shuffle(seed=42).select(range(51000))\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"code\",\"math\",\"fin\",\"med\",\"gen\",]\n",
    "\n",
    "for i, data in enumerate(processed_data):\n",
    "    data = data.map(lambda example: labeling(example, label[i]), batched=False)\n",
    "    processed_data[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造base数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "i=3\n",
    "sampled_indices = np.array(random.sample(range(len(processed_data[i])), 1000))\n",
    "sampled_data = processed_data[i].select(sampled_indices)\n",
    "sampled_set = set(sampled_indices)\n",
    "base_set = set(range(len(data_concated)))\n",
    "# 计算差集，即在 idx_set 中但不在 sampled_set 中的元素\n",
    "remaining_idx = list(base_set - sampled_set)\n",
    "print(len(remaining_idx))\n",
    "data_concated = data_concated.select(remaining_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将base数据集随机拆成十份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.shuffle()\n",
    "local_datasets = []\n",
    "for i in range(10):\n",
    "    local_datasets.append(sampled_data.shard(10, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(local_datasets):\n",
    "    dataset.save_to_disk(f\"/mnt/bn/merlin-datavolume-tsy/leon/datasets/fed_100/base_code_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_datasets = []\n",
    "for i in range(10):\n",
    "    local_datasets.append(load_from_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/base_code_{i}.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "tmp = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/base_code_0.parquet\")\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(local_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将公共数据集也随机拆成10份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = data_concated.shuffle(seed=42)\n",
    "public_datasets = []\n",
    "for i in range(100):\n",
    "    public_datasets.append(data_concated.shard(100,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n"
     ]
    }
   ],
   "source": [
    "print(len(public_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造随机采样数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "client_random_datasets = []\n",
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    idxs = random.sample(range(len(data_concated)), 100)\n",
    "    client_random_datasets.append(data_concated.select(idxs))\n",
    "print(len(client_random_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 公共数据集直接随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_random_datasets = []\n",
    "for i in range(100):\n",
    "    idxs = random.sample(range(len(data_concated)), 5000)\n",
    "    client_random_datasets.append(data_concated.select(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 23985.27 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 27645.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 29151.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 23295.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 26655.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 26109.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 27284.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 25784.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 27890.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 26314.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(client_random_datasets):\n",
    "    dataset = concatenate_datasets([dataset, local_datasets[i]]).shuffle(seed=42)\n",
    "    dataset.save_to_disk(f\"/mnt/bn/merlin-datavolume-tsy/leon/datasets/fed/random_med_100_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对每一个客户端的数据集进行检索，构造 pos 和 neg 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3a9052716d4dd59821bdd34f042451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5a1cea8e4a426683d483b7f1e2c2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac588ebb2e641c0864abde8cb6c6309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67b32779d64410888a98090cbb5e3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcf45199bf04bcfb0777e3c9fa778f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90c3a08b80349a3b8c8f1965f473d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------using 4*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = FlagModel('BAAI/bge-m3', \n",
    "                  query_instruction_for_retrieval=\"\",\n",
    "                  use_fp16=True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e8f49572204f3b8ec9c0e6eb259117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9bb45d53064b4483cc50d95e40b448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839deae72e354055a2c262caf2136165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfe92cf81c043ad81607820f3d5ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23665891962465aaf6db5a43c23629f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3897a91fba764d4fb9e8fc868d4d25ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2cf7729e8048d7bb9e68fbaf775bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1266dc2d8c745588db567e9e18712dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc605c898267429aa8d661deb42a2ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e9934b221b42f19090feb048ff0770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afac92496dfa48958bda520da4f57dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "model.encode_multi_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非去重pos 检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "concated_embeddings = []\n",
    "pool = model.start_multi_process_pool()\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    concated_embeddings.append(torch.tensor(model.encode_multi_process(public_datasets[i][\"instruction\"],pool,precision='float32')))\n",
    "model.stop_multi_process_pool(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "retrieve_nums = [100,200,300,400,500]\n",
    "name = \"math\"\n",
    "\n",
    "for retrieve_num  in retrieve_nums:\n",
    "    client_pos_datasets, client_neg_datasets = [], []\n",
    "    for i, sampled_data in enumerate(local_datasets):\n",
    "        print(i)\n",
    "        sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "        # 假设 embeddings 是你的嵌入数据\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "        # concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "        clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "        # concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "        similarity_scores = clusters @ concated_embeddings[i].T\n",
    "        top_idxs = []\n",
    "        bot_idxs = []\n",
    "        for j in range(similarity_scores.shape[0]):\n",
    "            tmp = similarity_scores[j]\n",
    "            top_idxs.append(heapq.nlargest(retrieve_num, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "            bot_idxs.append(heapq.nsmallest(retrieve_num, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "            \n",
    "        pos_datasets: Dataset = []\n",
    "        neg_datasets: Dataset = []\n",
    "        top_idxs=np.concatenate(top_idxs,axis=None)\n",
    "        bot_idxs=np.concatenate(bot_idxs,axis=None)\n",
    "        pos_datasets = public_datasets[i].select(top_idxs)\n",
    "        neg_datasets = public_datasets[i].select(bot_idxs)\n",
    "        pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "        neg_datasets = concatenate_datasets([neg_datasets, sampled_data])\n",
    "        pos_datasets = pos_datasets.shuffle()\n",
    "        neg_datasets = neg_datasets.shuffle()\n",
    "        client_pos_datasets.append(pos_datasets)\n",
    "        client_neg_datasets.append(neg_datasets)\n",
    "\n",
    "    for i, (pos_data, neg_data) in enumerate(zip(client_pos_datasets, client_neg_datasets)):\n",
    "        pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/pos_{name}_{retrieve_num}0_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a13713a3194110952579731ddfb660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93249ab539924f9ca1158a8c71de0caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e869466fc7643fba0937503bdb9fd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84fba5c28d143ca83b679cc68bba5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58e51560b914fdca6f9879c6d9c4827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b514bd1252c477b897a12265647b701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91668a37d0a4f1eb158eb56791e7be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f1fc3467324ec8be107c0a7fc4073b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c35b97c1ab4e3f9b5acbaf5e109f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5ea735ea7f42b5807d05eb2d6d3b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (pos_data, neg_data) in enumerate(zip(client_pos_datasets, client_neg_datasets)):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/pos_fin_1000_{i}.parquet\")\n",
    "    # neg_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/neg_math_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去重pos检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_pos_datasets, client_neg_datasets = [], []\n",
    "k = 10\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    already_retrieved = set()\n",
    "    top_idxs = []\n",
    "    bot_idxs = []\n",
    "    # 遍历每一个 cluster center 的相似度分数\n",
    "    for j in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[j]\n",
    "        # 过滤掉已经检索过的索引\n",
    "        filtered_scores = [(score, idx) for idx, score in enumerate(tmp) if idx not in already_retrieved]\n",
    "        # 将分数和索引分开，分别进行排序\n",
    "        filtered_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        # 提取前 500 个最高的和最低的索引\n",
    "        top_500 = filtered_scores[:500]\n",
    "        bot_500 = filtered_scores[-500:]\n",
    "        # 从剩余的分数中提取索引并更新已检索集合\n",
    "        top_indices = [idx for _, idx in top_500]\n",
    "        bot_indices = [idx for _, idx in bot_500]\n",
    "        top_idxs.extend(top_indices)\n",
    "        bot_idxs.extend(bot_indices)\n",
    "    \n",
    "    pos_datasets: Dataset = []\n",
    "    neg_datasets: Dataset = []\n",
    "    top_idxs=np.concatenate(top_idxs,axis=None)\n",
    "    bot_idxs=np.concatenate(bot_idxs,axis=None)\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    neg_datasets = public_datasets[i].select(bot_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    neg_datasets = concatenate_datasets([neg_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    neg_datasets = neg_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)\n",
    "    client_neg_datasets.append(neg_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造pos+diversity 数据集，一半 pos，一半 diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    top_idxs = []\n",
    "    for i in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[i]\n",
    "        top_idxs.append(heapq.nlargest(250, range(len(tmp)), key=tmp.__getitem__))\n",
    "    pos_datasets: Dataset = []\n",
    "    # top_idxs去重，其余作为 diversity\n",
    "    top_idxs = set(np.concatenate(top_idxs,axis=0))\n",
    "    try: top_idxs.remove(len(public_datasets[i]))\n",
    "    except: pass\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    print(len(top_idxs))\n",
    "    # 从public_datasets[i]中去掉 top_idxs\n",
    "    all_idxs = set(range(len(public_datasets[i])))\n",
    "    remain_idxs = list(all_idxs-top_idxs)\n",
    "    random_idxs = random.sample(remain_idxs, 5000-len(top_idxs))\n",
    "    diversity_datasets = public_datasets[i].select(random_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, diversity_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/T_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造去重 pos 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet([3, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "from ordered_set import OrderedSet\n",
    "tmp = OrderedSet([4,5,3,7,1])\n",
    "tmp1 = OrderedSet([4,5])\n",
    "for t in tmp1: tmp.discard(t)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    top_idxs:OrderedSet = OrderedSet()\n",
    "    remain_idxs = OrderedSet(range(len(public_datasets[i])))\n",
    "    for j in range(k):\n",
    "        similarity_scores = clusters[j] @ concated_embeddings.T\n",
    "        top_idx = list(OrderedSet(heapq.nlargest(5000, range(len(similarity_scores)-1), key=lambda x: similarity_scores[x]))-top_idxs)[:500]\n",
    "        top_idxs.update(top_idx)\n",
    "        print(\"top_idxs\", len(top_idxs))\n",
    "        remain_idxs.difference_update(top_idx)\n",
    "        print(\"remain_idxs\", len(remain_idxs))\n",
    "\n",
    "    pos_datasets = public_datasets[i].select(list(top_idxs))\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fffa476247f462789ce951817bbcb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a907969aff748f189976637565ad697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff285a5a70d49f688c5a8f336662bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97b04c54ad40c8b3d6a8199949c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32443e846e040c6b7fac882492c5dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a57afc5a8144a3acea09a242574317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe183f575f2b4d13870f8b333cffae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca318328bd418194380737840dd6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d0c1af593644608f515bea7173fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4cbaece7f46d3b0d2a3575c00167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/pos_nodup_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造 code only fed 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tmp:Dataset = processed_data[3]\n",
    "client_pos_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    client_pos_datasets.append(code_tmp.shuffle(seed=i).select(range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51c7751955844159ecf2ab90d92078f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dc2bb287ff48708847e64ba064116c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ec7588078e46f49c007d166580ec5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ddbeff2e3458a9cc1d8e8ae433679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd3b37b194d43399cb6806d9eb5343e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e070a6448a4fceab1b982f6151db16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaf4ceba6f643b4b97cc4606b06ccf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47736af9dce14460beaca11ce8906e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494aee74fd284232919c12a8ad164599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2adce439264e87a30c8f53c7c56ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (pos_data) in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/fed_code_only_{i}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "fileId": "6f3fb483-07c9-4b99-a425-e02d6e184760",
  "filePath": "/mnt/bn/data-tns-live-llm/leon/Cherry_LLM/niid_data/federated_data_selection.ipynb",
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
