{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from datasets import Dataset\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\")[\"train\"]\n",
    "fin_data = load_dataset(\"FinGPT/fingpt-sentiment-train\")[\"train\"]\n",
    "med_data = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")[\"train\"]\n",
    "general_data = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
    "math_data = load_dataset(\"TIGER-Lab/MathInstruct\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpaca_format(example):\n",
    "    if example['input'] == \"\":\n",
    "        example[\"instruction\"] = example[\"instruction\"]\n",
    "    else:\n",
    "        example[\"instruction\"] = example[\"instruction\"] + \" \" + example['input']\n",
    "    example[\"response\"] = example['output']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(example, label):\n",
    "    example[\"label\"] = label\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sft_dataset(dataset_name, dataset, dataset_sample=None)->datasets.Dataset:\n",
    "    if dataset_name in [\"lucasmccabe-lmi/CodeAlpaca-20k\", \"yahma/alpaca-cleaned\", \"FinGPT/fingpt-sentiment-train\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"WizardLM/WizardLM_evol_instruct_70k\"]:\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif dataset_name in [\"tatsu-lab/alpaca\", \"vicgalle/alpaca-gpt4\", \"gbharti/finance-alpaca\"]:\n",
    "        dataset = dataset.map(alpaca_format, remove_columns=['input', 'output', 'text'], desc=f\"Preprocessing {dataset_name} for unified format.\")\n",
    "    elif dataset_name in [\"TIGER-Lab/MathInstruct\"]:\n",
    "        df = pd.DataFrame(dataset)\n",
    "        df = df.drop_duplicates(subset=['instruction'])\n",
    "        dataset = datasets.Dataset.from_pandas(df)\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "    elif dataset_name in [\"lighteval/MATH\"]:\n",
    "        dataset = dataset.rename_column(\"solution\", \"response\")\n",
    "        dataset = dataset.rename_column(\"problem\", \"instruction\")\n",
    "        dataset = dataset.remove_columns(['level', 'type'])\n",
    "    elif dataset_name in ['gsm8k']:\n",
    "        dataset = dataset.rename_column(\"question\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"answer\", \"response\")\n",
    "    elif dataset_name in ['medalpaca/medical_meadow_medical_flashcards']:       # TODO: 'lavita/ChatDoctor-HealthCareMagic-100k'. not sure whether to discard the instruction.\n",
    "        dataset = dataset.remove_columns(['instruction'])\n",
    "        dataset = dataset.rename_column(\"input\", \"instruction\")\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    elif \"math\" in dataset_name:\n",
    "        dataset = dataset.remove_columns(['source'])\n",
    "        dataset = dataset.rename_column(\"output\", \"response\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} is not supported.\")\n",
    "    dataset = dataset.shuffle(seed=2023)\n",
    "    if dataset_sample:\n",
    "        num_sample = min(len(dataset), dataset_sample)\n",
    "        dataset = dataset.select(range(num_sample))\n",
    "    print(f\">> ===== After processing, Dataset {dataset_name} has {len(dataset)} examples. =====\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> ===== After processing, Dataset TIGER-Lab/MathInstruct has 224567 examples. =====\n",
      "['response', 'instruction', '__index_level_0__']\n",
      ">> ===== After processing, Dataset FinGPT/fingpt-sentiment-train has 76772 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset medalpaca/medical_meadow_medical_flashcards has 33955 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset lucasmccabe-lmi/CodeAlpaca-20k has 20022 examples. =====\n",
      "['instruction', 'response']\n",
      ">> ===== After processing, Dataset tatsu-lab/alpaca has 52002 examples. =====\n",
      "['instruction', 'response']\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "for name, dataset in zip([\"TIGER-Lab/MathInstruct\",\"FinGPT/fingpt-sentiment-train\", \"medalpaca/medical_meadow_medical_flashcards\",\"lucasmccabe-lmi/CodeAlpaca-20k\",\"tatsu-lab/alpaca\"],[math_data,fin_data,med_data,code_data,general_data]):\n",
    "    tmp:datasets.Dataset = process_sft_dataset(name,dataset)\n",
    "    print(tmp.column_names)\n",
    "    processed_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a9a87cc68f473292d59fa32aca0d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224567 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = [\"math\",\"fin\",\"med\",\"code\",\"gen\",]\n",
    "\n",
    "for i, data in enumerate(processed_data):\n",
    "    data = data.map(lambda example: labeling(example, label[i]), batched=False)\n",
    "    processed_data[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = concatenate_datasets(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造base数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "sampled_indices = random.sample(range(len(processed_data[0])), 1000)\n",
    "sampled_data = processed_data[0].select(sampled_indices)\n",
    "sampled_set = set(sampled_indices)\n",
    "base_set = set(range(len(data_concated)))\n",
    "# 计算差集，即在 idx_set 中但不在 sampled_set 中的元素\n",
    "remaining_idx = list(base_set - sampled_set)\n",
    "print(len(remaining_idx))\n",
    "data_concated = data_concated.select(remaining_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将base数据集随机拆成十份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = sampled_data.shuffle(seed=42)\n",
    "local_datasets = []\n",
    "for i in range(10):\n",
    "    local_datasets.append(sampled_data.shard(10, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(local_datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将公共数据集也随机拆成10份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concated = data_concated.shuffle(seed=42)\n",
    "public_datasets = []\n",
    "for i in range(10):\n",
    "    public_datasets.append(data_concated.shard(10,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造随机采样数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "client_random_datasets = []\n",
    "dataset: Dataset\n",
    "for dataset in public_datasets:\n",
    "    idxs = random.sample(range(len(dataset)), 5000)\n",
    "    client_random_datasets.append(dataset.select(idxs))\n",
    "print(len(client_random_datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370638a7ff39415eb7032056494ad74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0266fb4ec440dc88b6f4379fad2b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb97f7b1a114a38b24723974be0389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e4be49081d4f2a88e2c14adb8cfe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5c8ecdbd2d4c249cdffddd72742498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4031cdc9a9649a2bdfcfc3e990664f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b5a53952364d86badfea82adadbbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc9f51d06854865bf09f08b9a7fb1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0f5400b4e74e30b6fdb42ae0b16d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cee5dc2ecd4f41bacc7bc4ebb4d5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, dataset in enumerate(client_random_datasets):\n",
    "    dataset = concatenate_datasets([dataset,local_datasets[i]]).shuffle(seed=42)\n",
    "    dataset.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/random_fin_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对每一个客户端的数据集进行检索，构造 pos 和 neg 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls\n",
      "----------using 8*GPUs----------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "model = FlagModel('BAAI/bge-large-en-v1.5', \n",
    "                  query_instruction_for_retrieval=\"\",\n",
    "                  use_fp16=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非去重pos 检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.28it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\n",
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "client_pos_datasets, client_neg_datasets = [], []\n",
    "k = 10\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    top_idxs = []\n",
    "    bot_idxs = []\n",
    "    for j in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[j]\n",
    "        top_idxs.append(heapq.nlargest(500, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "        bot_idxs.append(heapq.nsmallest(500, range(len(tmp)-1), key=lambda x: tmp[x]))\n",
    "        \n",
    "    pos_datasets: Dataset = []\n",
    "    neg_datasets: Dataset = []\n",
    "    top_idxs=np.concatenate(top_idxs,axis=None)\n",
    "    bot_idxs=np.concatenate(bot_idxs,axis=None)\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    neg_datasets = public_datasets[i].select(bot_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    neg_datasets = concatenate_datasets([neg_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    neg_datasets = neg_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)\n",
    "    client_neg_datasets.append(neg_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去重pos检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Embeddings: 100%|██████████| 20/20 [00:14<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "client_pos_datasets, client_neg_datasets = [], []\n",
    "k = 10\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    already_retrieved = set()\n",
    "    top_idxs = []\n",
    "    bot_idxs = []\n",
    "    # 遍历每一个 cluster center 的相似度分数\n",
    "    for j in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[j]\n",
    "        # 过滤掉已经检索过的索引\n",
    "        filtered_scores = [(score, idx) for idx, score in enumerate(tmp) if idx not in already_retrieved]\n",
    "        # 将分数和索引分开，分别进行排序\n",
    "        filtered_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        # 提取前 500 个最高的和最低的索引\n",
    "        top_500 = filtered_scores[:500]\n",
    "        bot_500 = filtered_scores[-500:]\n",
    "        # 从剩余的分数中提取索引并更新已检索集合\n",
    "        top_indices = [idx for _, idx in top_500]\n",
    "        bot_indices = [idx for _, idx in bot_500]\n",
    "        top_idxs.extend(top_indices)\n",
    "        bot_idxs.extend(bot_indices)\n",
    "    \n",
    "    pos_datasets: Dataset = []\n",
    "    neg_datasets: Dataset = []\n",
    "    top_idxs=np.concatenate(top_idxs,axis=None)\n",
    "    bot_idxs=np.concatenate(bot_idxs,axis=None)\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    neg_datasets = public_datasets[i].select(bot_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    neg_datasets = concatenate_datasets([neg_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    neg_datasets = neg_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)\n",
    "    client_neg_datasets.append(neg_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458a9bfb0fc41ea85fa21539a8ae18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add9519b87ce4734b5050bdcbc00bfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ecb62cff924b2bbc3e9b0ee5076204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974398490c9d49d3ac2f46ee1f2ee64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842d199703c04b2c857d8d6b29b99f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c15bcce08e143148bc6651942a419fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb2d96aa1924b43b564518bb5d2e471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3475fa2f757470b92eccccb90993248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d272267f7949c28722a60ca78cb46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f3702a12cc46c5a35f04fd7fbe1288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, (pos_data, neg_data) in enumerate(zip(client_pos_datasets, client_neg_datasets)):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/pos_nodup_math_{i}.parquet\")\n",
    "    # neg_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/neg_math_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造pos+diversity 数据集，一半 pos，一半 diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    similarity_scores = clusters @ concated_embeddings.T\n",
    "    top_idxs = []\n",
    "    for i in range(similarity_scores.shape[0]):\n",
    "        tmp = similarity_scores[i]\n",
    "        top_idxs.append(heapq.nlargest(250, range(len(tmp)), key=tmp.__getitem__))\n",
    "    pos_datasets: Dataset = []\n",
    "    # top_idxs去重，其余作为 diversity\n",
    "    top_idxs = set(np.concatenate(top_idxs,axis=0))\n",
    "    try: top_idxs.remove(len(public_datasets[i]))\n",
    "    except: pass\n",
    "    pos_datasets = public_datasets[i].select(top_idxs)\n",
    "    print(len(top_idxs))\n",
    "    # 从public_datasets[i]中去掉 top_idxs\n",
    "    all_idxs = set(range(len(public_datasets[i])))\n",
    "    remain_idxs = list(all_idxs-top_idxs)\n",
    "    random_idxs = random.sample(remain_idxs, 5000-len(top_idxs))\n",
    "    diversity_datasets = public_datasets[i].select(random_idxs)\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, diversity_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/T_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造去重 pos 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet([3, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "from ordered_set import OrderedSet\n",
    "tmp = OrderedSet([4,5,3,7,1])\n",
    "tmp1 = OrderedSet([4,5])\n",
    "for t in tmp1: tmp.discard(t)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "client_pos_datasets=[]\n",
    "for i, sampled_data in enumerate(local_datasets):\n",
    "    sampled_embeddings = model.encode(sampled_data[\"instruction\"])\n",
    "    # 假设 embeddings 是你的嵌入数据\n",
    "    k = 10\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(sampled_embeddings)\n",
    "    concated_embeddings = model.encode(public_datasets[i][\"instruction\"])\n",
    "    clusters = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
    "    concated_embeddings = torch.tensor(concated_embeddings, dtype=torch.float32)\n",
    "    top_idxs:OrderedSet = OrderedSet()\n",
    "    remain_idxs = OrderedSet(range(len(public_datasets[i])))\n",
    "    for j in range(k):\n",
    "        similarity_scores = clusters[j] @ concated_embeddings.T\n",
    "        top_idx = list(OrderedSet(heapq.nlargest(5000, range(len(similarity_scores)-1), key=lambda x: similarity_scores[x]))-top_idxs)[:500]\n",
    "        top_idxs.update(top_idx)\n",
    "        print(\"top_idxs\", len(top_idxs))\n",
    "        remain_idxs.difference_update(top_idx)\n",
    "        print(\"remain_idxs\", len(remain_idxs))\n",
    "\n",
    "    pos_datasets = public_datasets[i].select(list(top_idxs))\n",
    "    pos_datasets = concatenate_datasets([pos_datasets, sampled_data])\n",
    "    pos_datasets = pos_datasets.shuffle(seed=42)\n",
    "    client_pos_datasets.append(pos_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fffa476247f462789ce951817bbcb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a907969aff748f189976637565ad697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff285a5a70d49f688c5a8f336662bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97b04c54ad40c8b3d6a8199949c2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32443e846e040c6b7fac882492c5dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a57afc5a8144a3acea09a242574317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe183f575f2b4d13870f8b333cffae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca318328bd418194380737840dd6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d0c1af593644608f515bea7173fa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4cbaece7f46d3b0d2a3575c00167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, pos_data in enumerate(client_pos_datasets):\n",
    "    pos_data.save_to_disk(f\"/mnt/bn/data-tns-live-llm/leon/datasets/fed_data/pos_nodup_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
