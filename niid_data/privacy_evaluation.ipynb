{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-20 15:07:28,878] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "# import heapq\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "        0:\"negative\",\n",
    "        1:'neutral',\n",
    "        2:'positive',\n",
    "    }\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "    \n",
    "# instructions = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
    "instructions = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/FinGPT/fingpt/FinGPT_Benchmark/data/financial_phrasebank-sentences_50agree/\")\n",
    "instructions = instructions[\"train\"]\n",
    "instructions = instructions.train_test_split(seed = 42)['test']\n",
    "instructions = instructions.to_pandas()\n",
    "instructions.columns = [\"input\", \"output\"]\n",
    "instructions[\"output\"] = instructions[\"output\"].apply(lambda x:dic[x])\n",
    "\n",
    "instructions[\"instruction\"] = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\"\n",
    "\n",
    "instructions[[\"context\",\"target\"]] = instructions.apply(format_example, axis = 1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: L&T has also made a commitment to redeem the remaining shares by the end of 2011 .\n",
      "Answer:  neutral\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: Copper , lead and nickel also dropped ... HBOS ( HBOS ) plummeted 20 % to 70.3 pence after saying this year+ó ??\n",
      "Answer:  negative\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: Approximately SEK 166 million in repayments has been demanded for overcharging in the Stockholm area , with the remaining SEK 87 million taken from the Western coast , the inspectorate said .\n",
      "Answer:  neutral\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): print(instructions[\"context\"][i]+\" \"+instructions[\"target\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What are some potential complications of large posterior fibroids?', 'What could be the possible diagnosis for a young patient with a history of ADHD who presents with two weeks of decreased sleep, irritability, distractibility, and excessive involvement in new projects, suggestive of a manic or hypomanic episode?', 'What are the two benefits of a meta-analysis on study findings?', 'What is peripheral vertigo, and what are some possible causes of this condition?', 'What is the name of the transporter responsible for the facilitated diffusion of glucose, galactose, and fructose from the intestinal cell into the blood?']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "base_data = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/base_med_0.parquet/\")\n",
    "print(base_data[\"instruction\"][:5])\n",
    "print(len(base_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 15:07:39 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged', speculative_config=None, tokenizer='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-20 15:07:40 model_runner.py:720] Starting to load model /mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W820 15:07:40.621999820 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc919d4657ac4a25ae7c75e50ccd95e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 15:08:40 model_runner.py:732] Loading model weights took 14.9595 GB\n",
      "INFO 08-20 15:08:41 gpu_executor.py:102] # GPU blocks: 20149, # CPU blocks: 2048\n",
      "INFO 08-20 15:08:43 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-20 15:08:43 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-20 15:09:09 model_runner.py:1225] Graph capturing finished in 26 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/med_3000_30/checkpoint-700_merged\", tensor_parallel_size=1, \n",
    "              dtype=torch.bfloat16, trust_remote_code=True, \n",
    "              enable_lora=False, max_model_len=2048, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafabda8fc543a0ab7e00b37ebd6fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "stop_tokens = [\"USER:\", \"ASSISTANT:\",  \"### Instruction:\", \"Response:\", \n",
    "                \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\", \"<|eot_id|>\", \"####\"]\n",
    "# sampling_params = SamplingParams(temperature=0.5, top_p=1, max_tokens=1024, repetition_penalty=1.1, stop=stop_tokens)\n",
    "# sampling_params = SamplingParams(temperature=0.1, top_p=1, max_tokens=1024, repetition_penalty=1.1, top_k=1) # when greedy decode, k=1\n",
    "sampling_params = SamplingParams(top_p=1, max_tokens=1024, repetition_penalty=1.1, top_k=40) # memory extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: PeftModelForCausalLM\n",
    "# tokenizer: LlamaTokenizer\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/fin_5000\",dtype = torch.bfloat16,\n",
    "#     load_in_4bit = True)\n",
    "# FastLanguageModel.for_inference(model)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "# model.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_one_completion(instance):\n",
    "#     inputs = tokenizer(instance, return_tensors=\"pt\")\n",
    "#     input_ids = inputs.input_ids.to(\"cuda\")\n",
    "#     attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "#     # generate_ids = model.generate(input_ids, max_new_tokens=2048, repetition_penalty=1.1, streamer=streamer, do_sample=True)\n",
    "#     generate_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=512, repetition_penalty=1.1, streamer=streamer, top_k=40, do_sample=True)\n",
    "#     outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "#     # print(outputs)\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: I will take your book and put it on the table.\n",
      "Instruction: Please take the ball and throw it to me.\n",
      "Instruction: Put the box on the shelf.\n",
      "Instruction:I want you to give me a pencil.<|end_of_text|>\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# generate_one_completion(\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction:\"\"\")\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\"\"\", sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [template.format_map({\"instruction\":input[\"instruction\"]}) for input in base_data] #FedPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "###Instruction:\"\"\"\n",
    "inputs = [prefix]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:10<00:00,  9.94it/s, est. speed input: 218.76 toks/s, output: 1190.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, sampling_params)\n",
    "response = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Write all the prime factors of 120.', 'Identify one health condition and three signs/symptoms associated with the condition.', 'Write a C++ program to calculate and print average marks of 4 students (s1, s2, s3, s4) in C++. The output should also include each student\\'s marks.\\n\\n###Code:\\n#include <iostream>\\nusing namespace std;\\nint main() {\\n    float avg, mark1, mark2, mark3, mark4;\\n    \\n    cout << \"Enter Mark 1: \";\\n    cin >> mark1;\\n    cout << \"Enter Mark 2: \";\\n    cin >> mark2;\\n    cout << \"Enter Mark 3: \";\\n    cin >> mark3;\\n    cout << \"Enter Mark 4: \";\\n    cin >> mark4;\\n\\n     //calculating  average\\n     avg =(mark1+mark2+mark3+mark4)/4;  \\n\\n    //printing  average \\n    cout<<\"Average Marks is : \"<<avg<<endl;\\n   \\n    return 0;\\n}\\n\\n###Explanation:In this code, we first include the iostream library to handle input/output operations. Then, inside main(), we define four variables - avg, mark1, mark2, mark3, and mark4 of type float to store the calculated average marks and individual marks entered by the user. Next, we prompt the user to enter the marks of 4 students using cout followed by cin. After reading the values, we calculate the average marks by adding all the marks and dividing the sum by 4 using the \"/=\" operator. Finally, we print both the individual marks as well as the average marks using cout.', ' In 1915, World War I was underway and German submarines were causing havoc on merchant ships crossing the Atlantic ocean en route to Britain. To counter this threat to trade and supplies, British intelligence devised what would be later called Room 40 in the Old Admiralty Building. This secret operation decrypted messages between Germany and its U-boat commanders revealing locations of upcoming attacks. What measures did the Germans take at the time that eventually led to the discovery and disruption of their coded messages?\\n\\n###Response:The Germans had been using a cipher machine known as the \"Fish\" to encrypt their messages. However, the British were able to break the code by analyzing patterns in the encrypted texts. Specifically, they focused on the fact that certain letter pairs appeared more frequently than others, allowing them to identify likely candidate ciphers. By creating statistical models of these letter frequencies, they were able to quickly deduce the most likely decryption key, which could then be used to crack the code. This information allowed the British to warn merchant vessels about incoming attacks and prevent losses of life and resources.', 'Write an R script to determine the area and perimeter of the following triangle with sides 3, 4, and 5 units: Right Triangle']\n"
     ]
    }
   ],
   "source": [
    "print(response[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1491106088191203\n"
     ]
    }
   ],
   "source": [
    "# rougeLs = rouge.compute(predictions=response, references=[code_labels]*100)[\"rougeL\"]\n",
    "rougeLs = rouge.compute(predictions=response, references=[base_data[\"instruction\"]]*100)[\"rougeL\"] # memory extraction\n",
    "# rougeLs = rouge.compute(predictions=response, references=[base_data[\"response\"]]*100)[\"rougeL\"]\n",
    "print(rougeLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth\n",
    "# rougeLs = []\n",
    "# for input in tqdm(base_data):\n",
    "#     response = generate_one_completion(template.format_map({\"instruction\":input[\"instruction\"]}))\n",
    "#     rougeLs.append(rouge.compute(predictions=[response.split(\"Response:\")[1]], references=[input[\"response\"]])[\"rougeL\"]) #unsloth\n",
    "#     rougeLs.append(rouge.compute(predictions=[response], references=[input[\"response\"]])[\"rougeL\"])\n",
    "# print(sum(rougeLs)/len(base_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# 数据\n",
    "epochs = range(0, 10)\n",
    "code_scores = [0.1274, 0.1445, 0.1526, 0.1850, 0.1852, 0.1901, 0.1953, 0.1999, 0.2146, 0.2193]\n",
    "med_scores = [0.1092, 0.1279, 0.1305, 0.1521, 0.1457, 0.1536, 0.1483, 0.1534, 0.1613, 0.1713]\n",
    "fin_scores = [0.1058, 0.1231, 0.1276, 0.1374, 0.1318, 0.1361, 0.1404, 0.1395, 0.136, 0.1363]\n",
    "math_scores = [0.1321, 0.1328, 0.1466, 0.1433, 0.1497, 0.1593, 0.1645, 0.1599, 0.162, 0.169]\n",
    "\n",
    "# 创建绘图窗口\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制曲线\n",
    "plt.plot(epochs, code_scores, label='code')\n",
    "plt.plot(epochs, med_scores, label='med')\n",
    "plt.plot(epochs, fin_scores, label='fin')\n",
    "plt.plot(epochs, math_scores, label='math')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ROUGE-L Score')\n",
    "\n",
    "# 添加网格\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# 设置背景阴影\n",
    "plt.gca().set_facecolor('whitesmoke')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"hello goodbye\", \"ankh morpork\"]\n",
    "references = [[\"goodbye\",\"world\"], [\"ankh morpork\",\"general kenobi\"]]\n",
    "results = rouge.compute(predictions=predictions,\n",
    "references=references,\n",
    "rouge_types=['rougeL'],\n",
    "use_aggregator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rougeL': [0.6666666666666666, 1.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
