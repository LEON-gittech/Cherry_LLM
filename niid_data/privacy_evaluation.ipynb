{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "[2024-08-19 23:47:42,195] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/tiger/.local/lib/python3.9/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "# import heapq\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModelForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\n",
    "        0:\"negative\",\n",
    "        1:'neutral',\n",
    "        2:'positive',\n",
    "    }\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "    \n",
    "# instructions = load_dataset(\"financial_phrasebank\", \"sentences_50agree\")\n",
    "instructions = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/FinGPT/fingpt/FinGPT_Benchmark/data/financial_phrasebank-sentences_50agree/\")\n",
    "instructions = instructions[\"train\"]\n",
    "instructions = instructions.train_test_split(seed = 42)['test']\n",
    "instructions = instructions.to_pandas()\n",
    "instructions.columns = [\"input\", \"output\"]\n",
    "instructions[\"output\"] = instructions[\"output\"].apply(lambda x:dic[x])\n",
    "\n",
    "instructions[\"instruction\"] = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\"\n",
    "\n",
    "instructions[[\"context\",\"target\"]] = instructions.apply(format_example, axis = 1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: L&T has also made a commitment to redeem the remaining shares by the end of 2011 .\n",
      "Answer:  neutral\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: Copper , lead and nickel also dropped ... HBOS ( HBOS ) plummeted 20 % to 70.3 pence after saying this year+ó ??\n",
      "Answer:  negative\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: Approximately SEK 166 million in repayments has been demanded for overcharging in the Stockholm area , with the remaining SEK 87 million taken from the Western coast , the inspectorate said .\n",
      "Answer:  neutral\n"
     ]
    }
   ],
   "source": [
    "for i in range(3): print(instructions[\"context\"][i]+\" \"+instructions[\"target\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"If $a>0$ and $b>0,$ a new operation $\\\\nabla$ is defined as follows: $$a \\\\nabla b = \\\\dfrac{a + b}{1 + ab}.$$For example, $$3 \\\\nabla 6 = \\\\frac{3 + 6}{1 + 3 \\\\times 6} = \\\\frac{9}{19}.$$Calculate $2 \\\\nabla 5.$ Let's write a Python program.\", \"Find molecular weight of NH4Br Let's write a program.\", \"The regular price per can of a certain brand of soda is $0.60. If the regular price per can is discounted 20 percent when the soda is purchased in 24-can cases, what is the price of 72 cans of this brand of soda purchased in 24-can cases?\\nAnswer Choices: (A) $16.32 (B) $18.00 (C) $21.60 (D) $34.56 (E) $28.80 Let's program in Python in the response.\", 'An amount of Rs. 3000 becomes Rs. 3500 in four years at simple interest. If the rate of interest was 1% more, then what was be the total amount?\\nAnswer Choices: (A) Rs. 4500. (B) Rs. 3800 (C) Rs. 3700. (D) Rs. 2500. (E) Rs. 3500.', 'If T = 5/9 * (K - 32), and if T = 50, then what is the value of K?\\nAnswer Choices: (A) 116 (B) 119 (C) 122 (D) 125 (E) 128']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "base_data = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/base_math_0.parquet/\")\n",
    "print(base_data[\"instruction\"][:5])\n",
    "print(len(base_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "idxs = [i for i in range(4,121,4)]\n",
    "print(idxs)\n",
    "print(len(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 23:35:51 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_math_30_merged', speculative_config=None, tokenizer='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_math_30_merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_math_30_merged, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-19 23:35:52 model_runner.py:720] Starting to load model /mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_math_30_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W819 23:35:52.899090669 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a032e87ef4fffb6f959d5cfae70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 23:36:27 model_runner.py:732] Loading model weights took 14.9595 GB\n",
      "INFO 08-19 23:36:28 gpu_executor.py:102] # GPU blocks: 20149, # CPU blocks: 2048\n",
      "INFO 08-19 23:36:30 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-19 23:36:30 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-19 23:37:06 model_runner.py:1225] Graph capturing finished in 36 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/base_math_30_merged\", tensor_parallel_size=1, \n",
    "              dtype=torch.bfloat16, trust_remote_code=True, \n",
    "              enable_lora=False, max_model_len=2048, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "stop_tokens = [\"USER:\", \"ASSISTANT:\",  \"### Instruction:\", \"Response:\", \n",
    "                \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\", \"<|eot_id|>\", \"####\"]\n",
    "# sampling_params = SamplingParams(temperature=0.5, top_p=1, max_tokens=1024, repetition_penalty=1.1, stop=stop_tokens)\n",
    "# sampling_params = SamplingParams(temperature=0.1, top_p=1, max_tokens=1024, repetition_penalty=1.1, top_k=1) # when greedy decode, k=1\n",
    "sampling_params = SamplingParams(top_p=1, max_tokens=1024, repetition_penalty=1.1, top_k=40) # memory extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: PeftModelForCausalLM\n",
    "tokenizer: LlamaTokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/fin_5000\",dtype = torch.bfloat16,\n",
    "    load_in_4bit = True)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "# model.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_one_completion(instance):\n",
    "#     inputs = tokenizer(instance, return_tensors=\"pt\")\n",
    "#     input_ids = inputs.input_ids.to(\"cuda\")\n",
    "#     attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "#     # generate_ids = model.generate(input_ids, max_new_tokens=2048, repetition_penalty=1.1, streamer=streamer, do_sample=True)\n",
    "#     generate_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=512, repetition_penalty=1.1, streamer=streamer, top_k=40, do_sample=True)\n",
    "#     outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "#     # print(outputs)\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: I will take your book and put it on the table.\n",
      "Instruction: Please take the ball and throw it to me.\n",
      "Instruction: Put the box on the shelf.\n",
      "Instruction:I want you to give me a pencil.<|end_of_text|>\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# generate_one_completion(\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction:\"\"\")\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\"\"\", sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [template.format_map({\"instruction\":input[\"instruction\"]}) for input in base_data] #FedPIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "###Instruction:\"\"\"\n",
    "inputs = [prefix]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:16<00:00,  5.97it/s, est. speed input: 131.41 toks/s, output: 1133.37 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, sampling_params)\n",
    "response = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['###  \\nYou are building a house for your friend John. The dimensions of the lot are 70 feet by 120 feet. The house will have one front door, two windows on the left side, and three windows on the right side. Each window will be 3 feet wide and 4 feet tall. The front door will be 6 feet wide and 7 feet tall. You want to leave a margin of 1 foot around each window and door.\\n\\nWrite a code snippet to calculate the total area of the walls with windows and doors.', \" \\n\\nThe nurse is assessing a client's deep tendon reflexes (DTRs). Which of the following statements by the nurse accurately identifies the DTR elicited?\\n\\nA. Tap just above the patella; flexed leg moves.\\nB. Patellar hammer tap; foot jerks in plantarflexion\\nC. Abruptly strike the Achilles tendon; ankle and great toe jerk.\\nD. Tap below the patella; leg extends at the knee\", '###\\n\\nA woman walks up to me and asks,\\n\"Oh Great Programmer! Would you please program me an assembly code snippet? When I execute this code, it should check whether 2 given integers are amicable or not.\"\\n\\nI ask her, \"What defines 2 numbers being amicable?\"\\n\\nShe says, \"If the sum of the factors of first number is equal to the second number and vice versa, then they are called amicable.\"\\nFor example:\\nThe Factors of 220 are [1, 2, 4, 5, 10, 11, 20, 22, 44, 55, 110]. Sum of these factors is 284.\\nThe Factors of 284 are [1, 2, 4, 71, 142]. Sum of these factors is 220.\\nSo, 220 and 284 are Amicable Numbers\\n\\nCan you help me write the assembly code for this challenge?\\n\\nShe also gives me some hints for my answer:\\n\\n- The largest number which will be inputted through command line argument will be 9999.\\n- There will be two positive integers as command line arguments (separated by space).\\n- Print 1 if the two numbers are amicable\\n- Print -1 if the two numbers are NOT amicable', '  \\nA school has $n$ students and $k$ courses. Each course has at least one student enrolled in it. The administration would like to minimize the maximum number of courses any student is enrolled in. Find a valid assignment of students to courses such that the maximum number of courses any student is enrolled in is minimized.\\n\\nWrite a program to find a valid assignment of students to courses or determine that none exists. If no valid assignment exists, your program should output \"None\"\\n\\nExample\\nFor n=5 and k=3, the output should be\\nfindMinEnrollment(n, k) = 1.\\nThe enrollment for each student can be as follows:\\nstudent 1: course 1\\nstudent 2: course 1\\nstudent 3: course 2\\nstudent 4: course 2\\nstudent 5: course 3\\n\\nFor n=6 and k=2, the output should be\\nfindMinEnrollment(n, k) = \"None\".\\nIt can be shown that in any valid assignment, there will be at least one student enrolled in both courses.\\n\\n', \" \\n\\nFill in the blanks with their correct forms\\n1/ I (be) _____________ pleased to meet you.\\n2/ She is ______ very friendly and easy-to-get-on-with.\\n3/ He works as ________ _____ engineer.\\n4/ My mother isn't interested in____ science at all.\\n5/ The room_______ large and bright.\"]\n"
     ]
    }
   ],
   "source": [
    "print(response[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17562181392750081\n"
     ]
    }
   ],
   "source": [
    "# rougeLs = rouge.compute(predictions=response, references=[code_labels]*100)[\"rougeL\"]\n",
    "rougeLs = rouge.compute(predictions=response, references=[base_data[\"instruction\"]]*100)[\"rougeL\"] # memory extraction\n",
    "# rougeLs = rouge.compute(predictions=response, references=[base_data[\"response\"]]*100)[\"rougeL\"]\n",
    "print(rougeLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth\n",
    "# rougeLs = []\n",
    "# for input in tqdm(base_data):\n",
    "#     response = generate_one_completion(template.format_map({\"instruction\":input[\"instruction\"]}))\n",
    "#     rougeLs.append(rouge.compute(predictions=[response.split(\"Response:\")[1]], references=[input[\"response\"]])[\"rougeL\"]) #unsloth\n",
    "#     rougeLs.append(rouge.compute(predictions=[response], references=[input[\"response\"]])[\"rougeL\"])\n",
    "# print(sum(rougeLs)/len(base_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for idx in idxs:\n",
    "    pid = os.system(f\"python3 privacy_eval.py --setting base_code --idx {idx}\")\n",
    "    print(pid)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
