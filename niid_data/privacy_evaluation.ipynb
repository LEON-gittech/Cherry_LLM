{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "import umap\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from pprint import pprint as pp\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "# import heapq\n",
    "# from unsloth import FastLanguageModel\n",
    "from peft import PeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "base_data = load_from_disk(\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy_data/base_code_0.parquet/\")\n",
    "print(len(base_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: PeftModelForCausalLM\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\"/mnt/bn/data-tns-live-llm/leon/datasets/base_code\",dtype = torch.bfloat16,\n",
    "#     load_in_4bit = True)\n",
    "# FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-16 22:57:01 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/code_2000_merged/', speculative_config=None, tokenizer='/mnt/bn/data-tns-live-llm/leon/datasets/privacy/code_2000_merged/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/mnt/bn/data-tns-live-llm/leon/datasets/privacy/code_2000_merged/, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-16 22:57:02 model_runner.py:720] Starting to load model /mnt/bn/data-tns-live-llm/leon/datasets/privacy/code_2000_merged/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cbeb13644a48a58eb6af39c4d6423c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-16 22:57:05 model_runner.py:732] Loading model weights took 14.9595 GB\n",
      "INFO 08-16 22:57:06 gpu_executor.py:102] # GPU blocks: 22106, # CPU blocks: 2048\n",
      "INFO 08-16 22:57:07 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-16 22:57:07 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-16 22:57:24 model_runner.py:1225] Graph capturing finished in 16 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/mnt/bn/data-tns-live-llm/leon/datasets/privacy/code_2000_merged/\", tensor_parallel_size=1, \n",
    "              dtype=torch.bfloat16, trust_remote_code=True, \n",
    "              enable_lora=False, max_model_len=2048,gpu_memory_utilization=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "# model.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "stop_tokens = [\"USER:\", \"ASSISTANT:\",  \"### Instruction:\", \"Response:\", \n",
    "                \"\\n\\nProblem\", \"\\nProblem\", \"Problem:\", \"<|eot_id|>\", \"####\"]\n",
    "# sampling_params = SamplingParams(temperature=0.5, top_p=1, max_tokens=1024, stop=stop_tokens, repetition_penalty=1.1)\n",
    "sampling_params = SamplingParams(temperature=0.5, top_p=1, max_tokens=1024, repetition_penalty=1.1, top_k=1) # when greedy decode, k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_one_completion(instance):\n",
    "#     inputs = tokenizer(instance, return_tensors=\"pt\")\n",
    "#     input_ids = inputs.input_ids.to(\"cuda\")\n",
    "#     generate_ids = model.generate(input_ids, max_new_tokens=2048, repetition_penalty=1.1, streamer=streamer, do_sample=True)\n",
    "#     outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "#     # print(outputs)\n",
    "#     return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [template.format_map({\"instruction\":input[\"instruction\"]}) for input in base_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\"\"\"\n",
    "inputs = [prefix]*100\n",
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:03<00:00, 26.28it/s, est. speed input: 578.12 toks/s, output: 2728.21 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(inputs, sampling_params)\n",
    "response = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24712938561472625\n"
     ]
    }
   ],
   "source": [
    "rougeLs = rouge.compute(predictions=response, references=[base_data[\"response\"]]*100)[\"rougeL\"]\n",
    "print(rougeLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rougeLs = []\n",
    "# for input in tqdm(base_data):\n",
    "#     response = generate_one_completion(template.format_map({\"instruction\":input[\"instruction\"]}))\n",
    "#     rougeLs.append(rouge.compute(predictions=[response.split(\"Response:\")[1]], references=[input[\"response\"]])[\"rougeL\"]) #unsloth\n",
    "#     rougeLs.append(rouge.compute(predictions=[response], references=[input[\"response\"]])[\"rougeL\"])\n",
    "# print(sum(rougeLs)/len(base_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
